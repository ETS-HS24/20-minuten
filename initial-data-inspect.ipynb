{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T14:39:16.897355Z",
     "start_time": "2024-10-16T14:39:16.889139Z"
    }
   },
   "source": [
    "raw_data_path = \"../data/20min-test-query-2020-jan-jun/raw-data/\"\n",
    "raw_file_name = \"cd558cca-53cb-4ff9-a3f5-89f70e139051__2024_10_13T18_23_54.tsv\"\n",
    "intermediate_data_path = \"../data/20min-test-query-2020-jan-jun/intermediate-data/\""
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T14:39:18.404541Z",
     "start_time": "2024-10-16T14:39:17.608308Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T14:39:20.769144Z",
     "start_time": "2024-10-16T14:39:19.383370Z"
    }
   },
   "source": [
    "with open(os.path.join(raw_data_path, raw_file_name)) as file:\n",
    "    raw_file = file.readlines()\n",
    "\n",
    "print(raw_file[0])\n",
    "print(raw_file[1].replace(\"\\t\", \" <tab> \"))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\tpubtime\tmedium_code\tmedium_name\trubric\tregional\tdoctype\tdoctype_description\tlanguage\tchar_count\tdateline\thead\tsubhead\tarticle_link\tcontent_id\tcontent\n",
      "\n",
      "38899273 <tab> 2020-02-10 08:28:00+01 <tab> ZWAO <tab> 20 minuten online <tab>  <tab>  <tab> WWE <tab> Online medium <tab> de <tab> 562 <tab> Singapore Airlines <tab> A380 muss wegen Orkantief durchstarten <tab>  <tab> https://www.20min.ch/schweiz/zuerich/story/24250135 <tab> 00028ac0-46af-cb40-908b-dedebaf3a9ea <tab> <tx><ld><p>Sturmtief Sabine wirbelt den Plan des Flughafen Zürich durcheinander. Ein Video eines Leser-Reporters zeigt den A380, der die Landung abbrechen muss.</p></ld><p>Der A380 der Singapore Airline musste den Landeanflug auf den Flughafen Kloten abbrechen. Beim zweiten Mal hat es geklappt wie ein Video eines Leser-Reporters zeigt.</p><p>Nicht nur der A380-Gigant musste durchstarten. Wie ein Leser-Reporter berichtet, traf es auch eine A330-300 der Swiss. Der Flieger kam von Tel Aviv.<br/><br/></p><p>(fss)</p></tx>\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T14:39:23.961054Z",
     "start_time": "2024-10-16T14:39:22.678961Z"
    }
   },
   "source": [
    "df = pd.read_csv(os.path.join(raw_data_path, raw_file_name), sep=\"\\t\")\n",
    "print(df.shape)\n",
    "print(df.head(1).transpose())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23555, 16)\n",
      "                                                                     0\n",
      "id                                                            38899273\n",
      "pubtime                                         2020-02-10 08:28:00+01\n",
      "medium_code                                                       ZWAO\n",
      "medium_name                                          20 minuten online\n",
      "rubric                                                             NaN\n",
      "regional                                                           NaN\n",
      "doctype                                                            WWE\n",
      "doctype_description                                      Online medium\n",
      "language                                                            de\n",
      "char_count                                                         562\n",
      "dateline                                            Singapore Airlines\n",
      "head                            A380 muss wegen Orkantief durchstarten\n",
      "subhead                                                            NaN\n",
      "article_link         https://www.20min.ch/schweiz/zuerich/story/242...\n",
      "content_id                        00028ac0-46af-cb40-908b-dedebaf3a9ea\n",
      "content              <tx><ld><p>Sturmtief Sabine wirbelt den Plan d...\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T14:39:26.008042Z",
     "start_time": "2024-10-16T14:39:25.977988Z"
    }
   },
   "source": [
    "print(df.info())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23555 entries, 0 to 23554\n",
      "Data columns (total 16 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   id                   23555 non-null  int64  \n",
      " 1   pubtime              23555 non-null  object \n",
      " 2   medium_code          23555 non-null  object \n",
      " 3   medium_name          23555 non-null  object \n",
      " 4   rubric               0 non-null      float64\n",
      " 5   regional             0 non-null      float64\n",
      " 6   doctype              23555 non-null  object \n",
      " 7   doctype_description  23555 non-null  object \n",
      " 8   language             23555 non-null  object \n",
      " 9   char_count           23555 non-null  int64  \n",
      " 10  dateline             23267 non-null  object \n",
      " 11  head                 23555 non-null  object \n",
      " 12  subhead              0 non-null      float64\n",
      " 13  article_link         23554 non-null  object \n",
      " 14  content_id           23555 non-null  object \n",
      " 15  content              23555 non-null  object \n",
      "dtypes: float64(3), int64(2), object(11)\n",
      "memory usage: 2.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T14:39:31.305975Z",
     "start_time": "2024-10-16T14:39:31.291980Z"
    }
   },
   "source": [
    "drop_nan_columns = [\"rubric\", \"regional\", \"subhead\"]\n",
    "df = df.drop(drop_nan_columns, axis=1)\n",
    "df.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23555, 13)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T14:39:33.458392Z",
     "start_time": "2024-10-16T14:39:33.439174Z"
    }
   },
   "source": "print(df.info())",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23555 entries, 0 to 23554\n",
      "Data columns (total 13 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   id                   23555 non-null  int64 \n",
      " 1   pubtime              23555 non-null  object\n",
      " 2   medium_code          23555 non-null  object\n",
      " 3   medium_name          23555 non-null  object\n",
      " 4   doctype              23555 non-null  object\n",
      " 5   doctype_description  23555 non-null  object\n",
      " 6   language             23555 non-null  object\n",
      " 7   char_count           23555 non-null  int64 \n",
      " 8   dateline             23267 non-null  object\n",
      " 9   head                 23555 non-null  object\n",
      " 10  article_link         23554 non-null  object\n",
      " 11  content_id           23555 non-null  object\n",
      " 12  content              23555 non-null  object\n",
      "dtypes: int64(2), object(11)\n",
      "memory usage: 2.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T14:39:35.745775Z",
     "start_time": "2024-10-16T14:39:35.738200Z"
    }
   },
   "cell_type": "code",
   "source": "df['content']",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        <tx><ld><p>Sturmtief Sabine wirbelt den Plan d...\n",
       "1        <tx><ld><p>Les voisins chanteurs de la rotonde...\n",
       "2        <tx><ld><p>Der Kanton Aargau führt im Kampf ge...\n",
       "3        <tx><ld><p>Inmitten der anderen Automobile der...\n",
       "4        <tx><ld><p>Le ministre de la santé Olivier Vér...\n",
       "                               ...                        \n",
       "23550    <tx><ld><p>La collecte de sapins de Noël a été...\n",
       "23551    <tx><ld><p>Un porte-parole du Département fédé...\n",
       "23552    <tx><ld><p>L'incendie d'un véhicule s'est prop...\n",
       "23553    <tx><ld><p>Comme l'épidémie de Covid-19 se pou...\n",
       "23554    <tx><ld><p>Tandis que certains ont critiqué le...\n",
       "Name: content, Length: 23555, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# EXPERIMENTS"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T15:33:39.461333Z",
     "start_time": "2024-10-16T15:33:39.449320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_data(data_array):\n",
    "    texts = []\n",
    "    for data in data_array:\n",
    "        texts.append(data)\n",
    "        \n",
    "    return texts\n",
    "\n",
    "data = prepare_data(df['content'].to_numpy())\n",
    "print(data[-3:])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<tx><ld><p>L\\'incendie d\\'un véhicule s\\'est propagé à la forêt environnante, samedi soir. Heureusement, les pompiers ont rapidement pu le maîtriser.</p></ld><p>Samedi soir, peu après 20h30, plusieurs lecteurs signalaient un incendie sur la route de la Forclaz, au-dessus de Martigny-Combe (VS). Dépêchés sur place, les secours ont constaté qu\\'il s\\'agissait d\\'une voiture qui était sortie de route et qui avait pris feu. Ses deux occupants n\\'ont heureusement été que légèrement blessés.</p><p>L\\'incendie s\\'est propagé à la forêt environnante. Les pompiers ont pu éviter qu\\'il prenne une ampleur trop importante et ont pu le maîtriser. Les autorités ont rappelé ces derniers jours que le risque d\\'incendie de forêt était élevé en raison des faibles précipitations des dernières semaines ainsi que des températures anormalement élevées ces derniers jours. Par exemple, dans le canton de Neuchâtel, les pompiers ont déjà été mobilisés plus de 10 fois en quelques jours pour des départs de feu.</p><ka><p>19h50 - Rte de la Forclaz, 200 mètres en amont croisement Ravoire, sortie de route d\\'une voiture dans la forêt, véhicule a pris feu et embrasé les arbres alentours. Deux occupants légèrement blessés. Police cantonale et PM Martigny-Combe, CSI-A Martigny &amp; environs engagés.<a href=\"https://t.co/m9Ts3ZCFoj\">pic.twitter.com/m9Ts3ZCFoj</a>\\x96 Police Valais (@PoliceValais)<a href=\"https://twitter.com/PoliceValais/status/1249065956581785601?ref_src=twsrc%5Etfw\">April 11, 2020</a></p></ka><p>(ywe/ats)</p></tx>', '<tx><ld><p>Comme l\\'épidémie de Covid-19 se poursuit, les organisateurs du concours de la chanson ont décidé de renoncer à la manifestation.</p></ld><p>La prochaine édition du concours Eurovision de la chanson, qui devait se tenir en mai à Rotterdam aux Pays-Bas, a été annulée. Une décision prise en raison de la pandémie de coronavirus, ont annoncé les organisateurs.</p><p>Plus tôt dans le mois, l\\'organisation du concours avait pourtant déclaré que son objectif était d\\'assurer le déroulement de cet évènement musical majeur malgré la crise actuelle du coronavirus.</p><p>Les Pays-Bas avaient gagné le droit d\\'organiser l\\'édition 2020 de l\\'Eurovision après la victoire du chanteur néerlandais Duncan Laurence au précédent concours, tenu en 2019 à Tel Aviv. La ville portuaire de Rotterdam avait été choisie en août pour accueillir la compétition.</p><ka><p>An official statement from the European Broadcasting Union on the<a href=\"https://twitter.com/hashtag/Eurovision?src=hash&amp;ref_src=twsrc%5Etfw\">#Eurovision</a>Song Contest 2020.<a href=\"https://t.co/b3h7akxvpF\">pic.twitter.com/b3h7akxvpF</a>\\x96 Eurovision Song Contest (@Eurovision)<a href=\"https://twitter.com/Eurovision/status/1240267936981569538?ref_src=twsrc%5Etfw\">March 18, 2020</a></p></ka><p>(nxp/ats)</p></tx>', \"<tx><ld><p>Tandis que certains ont critiqué le fait de pouvoir se mêler de la politique de législature décidée par le Conseil fédéral, des sénateurs ont demandé que la crise du coronavirus soit prise en compte.</p></ld><lg><p>La socialiste tessinoise Marina Carobbio Guscetti a notamment demandé que, dans les trois prochaines années, la loi sur les épidémies soit revue.KEYSTONE/Anthony Anex</p></lg><p>Le Conseil fédéral ne doit pas oublier les conséquences de la pandémie de Covid-19 dans son programme de législature. Élaborée avant la crise, la feuille de route du gouvernement ne tient pas compte de la situation actuelle. Le Conseil des Etats y a apporté lundi quelques adaptations.</p><p>Le programme du Conseil fédéral et de l'administration pour les années 2019 à 2023 se compose de trois lignes directrices. La Suisse assure durablement sa prospérité et saisit les chances qu’offre le numérique. Elle soutient la cohésion nationale et oeuvre au renforcement de la coopération internationale. Elle assure la sécurité, s’engage pour la protection du climat et agit en partenaire fiable sur le plan international.</p><p>Ces trois lignes directrices présentées en début d'année restent importantes, même dans la situation actuelle et après l'expérience de ces trois derniers mois, a relevé la présidente de la Confédération Simonetta Sommaruga. Le Conseil fédéral a fixé 18 objectifs et 53 mesures pour les atteindre. Ils doivent être poursuivis.</p><zt>Loi sur les épidémies à revoir</zt><p>La crise du coronavirus étant survenue entre-temps, les sénateurs ont décidé de charger le Conseil fédéral d'intégrer les enseignements tirés de la pandémie de Covid-19 dans son action. Il faut une vision globale, a précisé Marina Carobbio (PS/TI) au nom de la commission. Le Conseil des Etats veut notamment que la loi sur les épidémies soit modifiée en prenant en compte le coronavirus.</p><p>Plusieurs autres précisions ont été apportées au programme gouvernemental. La protection du climat et les effets des changements climatiques font partie des grands défis de la 51e législature. Le Conseil des Etats demande une stratégie, incluant aussi la protection des ressources naturelles.</p><p>Le carnet de route accorde aussi une large place à la politique européenne. Le Conseil fédéral prévoit ainsi d'adopter le message sur l'accord institutionnel avec l'UE d'ici à la fin de la législature. Marco Chiesa (UDC/TI) aurait souhaité biffer cet objectif, prônant la voie bilatérale. Par 24 voix contre 15, le conseil ne l'a pas suivi.</p><p>Pour le reste, le Conseil fédéral est appelé à consolider les relations économiques avec le Royaume-Uni. Dans le domaine migratoire, il devra prendre des décisions concernant la reprise de plusieurs développements des acquis de Schengen et de Dublin.</p><zt>Gouvernance numérique</zt><p>Concernant la numérisation, autre grand objectif annoncé par le gouvernement, les sénateurs demandent une stratégie visant une forte gouvernance numérique. Ils préconisent aussi un accès sans entraves au digital.</p><p>La promotion de la cohésion des régions et de la compréhension entre les cultures et les communautés linguistiques est aussi essentielle, a noté Marina Carobbio. Le Conseil des Etats a décidé d'ajouter, par 26 voix contre 15, un plan d'action avec les cantons pour la promotion du plurilinguisme et de cours de langue, comme l'a proposé Charles Juillard (PDC/JU). Il n'a en revanche pas voulu d'un plan d'action contre la discrimination.</p><p>Afin de créer un environnement économique stable, le gouvernement devra encore présenter un projet permettant d'accroître le dynamisme de la place économique helvétique. Côté santé, la Suisse devrait se munir d'un système de prévention efficace.</p><zt>Ingérance critiquée</zt><p>Plusieurs sénateurs ont critiqué la procédure entourant le programme de législature et le fait que le Parlement puisse modifier le contenu de l'agenda du Conseil fédéral. Cet exercice est chronophage et contraignant, a estimé Olivier Français (PLR/VD). Il prend beaucoup de temps tant en commission qu'en plénum, a-t-il regretté.</p><p>Le Parlement devrait seulement prendre acte du document, a renchéri Damian Müller (PLR/VD). La commission spéciale qui examine le programme devrait être abolie ces prochains mois, a-t-il proposé.</p><p>«C'est un moment important pour influencer l'action future du gouvernement», a justifié Carlo Sommaruga, auteur de plusieurs propositions rejetées. Le socialiste genevois aurait notamment souhaité intégrer au programme les objectifs de développement durable de l'Agenda 2030 de l'ONU.</p><p>En 2018, la commission des institutions politiques du Conseil national avait abandonné l'idée de réformer la procédure, après un avis négatif du Conseil fédéral.</p><p>Le dossier passe au Conseil national.</p><p>(ATS)</p></tx>\"]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "sind polizei des in sonderverordnung für zur und der die\n",
      "Topic 1:\n",
      "lendemain une italie instruments inspirés minutes leur br tx ld\n",
      "Topic 2:\n",
      "qui rotonde saint 15 et le les ont la de\n",
      "Topic 3:\n",
      "lendemain une italie instruments inspirés minutes leur br tx ld\n",
      "Topic 4:\n",
      "lendemain une italie instruments inspirés minutes leur br tx ld\n"
     ]
    }
   ],
   "execution_count": 15,
   "source": [
    "from Swissdox.BartTopicModel import BartTopicModel\n",
    "\n",
    "bart_topic_model = BartTopicModel(5)\n",
    "lda = bart_topic_model.extract_topics(data[1:3])\n",
    "bart_topic_model.print_topics(lda, 10)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T15:38:15.081478Z",
     "start_time": "2024-10-16T15:38:08.139005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from Swissdox.BartTopicModeling import BartTopicModeling\n",
    "\n",
    "bart = BartTopicModeling()\n",
    "print(data[0:1])\n",
    "print('---')\n",
    "bart.generate_topics(data[0:1], num_topics=5, max_length=100)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<tx><ld><p>Sturmtief Sabine wirbelt den Plan des Flughafen Zürich durcheinander. Ein Video eines Leser-Reporters zeigt den A380, der die Landung abbrechen muss.</p></ld><p>Der A380 der Singapore Airline musste den Landeanflug auf den Flughafen Kloten abbrechen. Beim zweiten Mal hat es geklappt wie ein Video eines Leser-Reporters zeigt.</p><p>Nicht nur der A380-Gigant musste durchstarten. Wie ein Leser-Reporter berichtet, traf es auch eine A330-300 der Swiss. Der Flieger kam von Tel Aviv.<br/><br/></p><p>(fss)</p></tx>']\n",
      "---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<tx><ld><p>Sturmtief Sabine wirbelt den Plan des Flughafen Zürich durcheinander. Ein Video eines Leser-Reporters zeigt den A380, der die Landung abbrechen muss.</p></ld></p>.<br/><br/><p>(fss)</p>:Der Flieger der Singapore Airline musste den Landeanflugbau auf den']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Some staff, not working"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "The pyarrow installation is not built with support for the Parquet file format (DLL load failed while importing _parquet: Die angegebene Prozedur wurde nicht gefunden.)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BartForConditionalGeneration, BartTokenizer, Trainer, TrainingArguments\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_dataset\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# Load the BART model and tokenizer\u001B[39;00m\n\u001B[0;32m      6\u001B[0m model_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfacebook/bart-base\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\datasets\\__init__.py:17\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# See the License for the specific language governing permissions and\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[0;32m     15\u001B[0m __version__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m3.0.1\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m---> 17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01marrow_dataset\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Dataset\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01marrow_reader\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ReadInstruction\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbuilder\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ArrowBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\datasets\\arrow_dataset.py:76\u001B[0m\n\u001B[0;32m     73\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcontrib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconcurrent\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m thread_map\n\u001B[0;32m     75\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m config\n\u001B[1;32m---> 76\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01marrow_reader\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ArrowReader\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01marrow_writer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ArrowWriter, OptimizedTypedSequence\n\u001B[0;32m     78\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata_files\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m sanitize_patterns\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\datasets\\arrow_reader.py:27\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TYPE_CHECKING, List, Optional, Union\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpyarrow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpa\u001B[39;00m\n\u001B[1;32m---> 27\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpyarrow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mparquet\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpq\u001B[39;00m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcontrib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconcurrent\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m thread_map\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdownload\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdownload_config\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DownloadConfig  \u001B[38;5;66;03m# noqa: F401\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\parquet\\__init__.py:20\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Licensed to the Apache Software Foundation (ASF) under one\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# or more contributor license agreements.  See the NOTICE file\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# distributed with this work for additional information\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     17\u001B[0m \n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# flake8: noqa\u001B[39;00m\n\u001B[1;32m---> 20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\parquet\\core.py:35\u001B[0m\n\u001B[0;32m     33\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpyarrow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_parquet\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01m_parquet\u001B[39;00m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[1;32m---> 35\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[0;32m     36\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe pyarrow installation is not built with support \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     37\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfor the Parquet file format (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mstr\u001B[39m(exc)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     38\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyarrow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_parquet\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (ParquetReader, Statistics,  \u001B[38;5;66;03m# noqa\u001B[39;00m\n\u001B[0;32m     41\u001B[0m                               FileMetaData, RowGroupMetaData,\n\u001B[0;32m     42\u001B[0m                               ColumnChunkMetaData,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     46\u001B[0m                               FileDecryptionProperties,\n\u001B[0;32m     47\u001B[0m                               SortingColumn)\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyarrow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfs\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (LocalFileSystem, FileSystem, FileType,\n\u001B[0;32m     49\u001B[0m                         _resolve_filesystem_and_path, _ensure_filesystem)\n",
      "\u001B[1;31mImportError\u001B[0m: The pyarrow installation is not built with support for the Parquet file format (DLL load failed while importing _parquet: Die angegebene Prozedur wurde nicht gefunden.)"
     ]
    }
   ],
   "execution_count": 18,
   "source": [
    "import torch\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the BART model and tokenizer\n",
    "model_name = 'facebook/bart-base'\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load a dataset (e.g., the CNN/DailyMail dataset)\n",
    "dataset = load_dataset('cnn_dailymail', '3.0.0', split='train[:1%]')\n",
    "\n",
    "# Preprocess the dataset\n",
    "def preprocess_function(examples):\n",
    "    inputs = [doc for doc in examples['article']]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding='max_length')\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Fine-tune the model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Generate topics\n",
    "def generate_topics(texts, model, tokenizer):\n",
    "    inputs = tokenizer(texts, return_tensors='pt', max_length=1024, truncation=True, padding='max_length')\n",
    "    summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=5, early_stopping=True)\n",
    "    topics = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\n",
    "    return topics\n",
    "\n",
    "# Example usage\n",
    "texts = [\"Your text data here.\"]\n",
    "topics = generate_topics(texts, model, tokenizer)\n",
    "print(topics)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danielritter\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pytorch_model.bin:  12%|#2        | 126M/1.02G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0e03f63c6309433a824fed55e0ea7021"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8d744d2c0620494ea3dcc85cd71ddd80"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7c6e5fb9b5034ff5b6edb77c1df6b35d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "82bccf1bdb004271be9412a137617a99"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Storm Sab']\n"
     ]
    }
   ],
   "execution_count": 19,
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the BART model and tokenizer\n",
    "model_name = 'facebook/bart-large'\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate topics\n",
    "def generate_topics(texts, model, tokenizer, num_beams=4, max_length=5):\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize the input texts\n",
    "    inputs = tokenizer(texts, return_tensors='pt', max_length=1024, truncation=True, padding='max_length')\n",
    "    \n",
    "    # Generate summaries (topics)\n",
    "    with torch.no_grad():\n",
    "        summary_ids = model.generate(inputs['input_ids'], num_beams=num_beams, max_length=max_length, early_stopping=True)\n",
    "    \n",
    "    # Decode the generated summaries\n",
    "    topics = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\n",
    "    return topics\n",
    "\n",
    "# Example usage\n",
    "texts = [\"Storm Sabine throws Zurich Airport's plan into disarray. A video from a reader-reporter shows the A380 having to abort the landing. The Singapore Airline A380 had to abort its approach to Kloten Airport. It worked the second time, as a video from a reader-reporter shows. It wasn't just the A380 giant that had to take off. As a reader reporter reports, a Swiss A330-300 was also hit. The plane came from Tel Aviv.\"]\n",
    "topics = generate_topics(texts, model, tokenizer)\n",
    "print(topics)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Storm Sabine throws Zurich Airport's plan into disarray. The Singapore Airline A380 had to abort its approach to Kloten Airport. It wasn't just the A380 giant that had to take off. As a reporter reports, a Swiss A330-300 was also hit. It worked the second time, as a video from a reporter shows. The plane came from Tel Aviv. The A380 was on its way to Zurich Airport when the storm hit. A video from\"]\n"
     ]
    }
   ],
   "execution_count": 21,
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the BART model and tokenizer\n",
    "model_name = 'facebook/bart-large'\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Simplify and clean the text\n",
    "    text = text.replace(\"reader-reporter\", \"reporter\")\n",
    "    return text\n",
    "\n",
    "# Function to generate topics\n",
    "def generate_topics(texts, model, tokenizer, num_beams=4, max_length=100):\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Preprocess the texts\n",
    "    texts = [preprocess_text(text) for text in texts]\n",
    "    \n",
    "    # Tokenize the input texts\n",
    "    inputs = tokenizer(texts, return_tensors='pt', max_length=1024, truncation=True, padding='max_length')\n",
    "    \n",
    "    # Generate summaries (topics)\n",
    "    with torch.no_grad():\n",
    "        summary_ids = model.generate(inputs['input_ids'], num_beams=num_beams, max_length=max_length, early_stopping=True)\n",
    "    \n",
    "    # Decode the generated summaries\n",
    "    topics = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\n",
    "    return topics\n",
    "\n",
    "# Example usage\n",
    "texts = [\"Storm Sabine throws Zurich Airport's plan into disarray. A video from a reporter shows the A380 having to abort the landing. The Singapore Airline A380 had to abort its approach to Kloten Airport. It worked the second time, as a video from a reporter shows. It wasn't just the A380 giant that had to take off. As a reporter reports, a Swiss A330-300 was also hit. The plane came from Tel Aviv.\"]\n",
    "topics = generate_topics(texts, model, tokenizer)\n",
    "print(topics)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 48,
   "source": [
    "from tweetopic import DMM\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Creating a vectorizer for extracting document-term matrix from the\n",
    "# text corpus.\n",
    "vectorizer = CountVectorizer(min_df=1, max_df=100)\n",
    "\n",
    "# Creating a Dirichlet Multinomial Mixture Model with 30 components\n",
    "dmm = DMM(n_components=30, n_iterations=100, alpha=0.1, beta=0.1)\n",
    "\n",
    "# Creating topic pipeline\n",
    "pipeline = Pipeline([\n",
    "    (\"vectorizer\", vectorizer),\n",
    "    (\"dmm\", dmm),\n",
    "])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing components.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 100/100 [00:00<00:00, 48964.56it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer', CountVectorizer(max_df=100)),\n",
       "                ('dmm', DMM(n_components=30, n_iterations=100))])"
      ],
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;, CountVectorizer(max_df=100)),\n",
       "                (&#x27;dmm&#x27;, DMM(n_components=30, n_iterations=100))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;, CountVectorizer(max_df=100)),\n",
       "                (&#x27;dmm&#x27;, DMM(n_components=30, n_iterations=100))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(max_df=100)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DMM</label><div class=\"sk-toggleable__content\"><pre>DMM(n_components=30, n_iterations=100)</pre></div></div></div></div></div></div></div>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49,
   "source": [
    "texts = [\"Storm Sabine throws Zurich Airport's plan into disarray. A video from a reporter shows the A380 having to abort the landing. The Singapore Airline A380 had to abort its approach to Kloten Airport. It worked the second time, as a video from a reporter shows. It wasn't just the A380 giant that had to take off. As a reporter reports, a Swiss A330-300 was also hit. The plane came from Tel Aviv.\"]\n",
    "pipeline.fit(texts)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Either corpus and model or topic_data has to be specified.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[50], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtopicwizard\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m topicwizard\u001B[38;5;241m.\u001B[39mvisualize(vectorizer\u001B[38;5;241m=\u001B[39mvectorizer, topic_model\u001B[38;5;241m=\u001B[39mdmm, corpus\u001B[38;5;241m=\u001B[39mtexts)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\topicwizard\\app.py:254\u001B[0m, in \u001B[0;36mvisualize\u001B[1;34m(corpus, model, topic_data, document_names, exclude_pages, group_labels, port, **kwargs)\u001B[0m\n\u001B[0;32m    252\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m topic_data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    253\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (model \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mor\u001B[39;00m (corpus \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m--> 254\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[0;32m    255\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEither corpus and model or topic_data has to be specified.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    256\u001B[0m         )\n\u001B[0;32m    257\u001B[0m     topic_data \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mprepare_topic_data(corpus, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    258\u001B[0m exclude_pages \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m() \u001B[38;5;28;01mif\u001B[39;00m exclude_pages \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mset\u001B[39m(exclude_pages)\n",
      "\u001B[1;31mTypeError\u001B[0m: Either corpus and model or topic_data has to be specified."
     ]
    }
   ],
   "execution_count": 50,
   "source": [
    "import topicwizard\n",
    "\n",
    "topicwizard.visualize(vectorizer=vectorizer, topic_model=dmm, corpus=texts)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'is_torch_npu_available' from 'transformers' (C:\\Users\\danielritter\\anaconda3\\Lib\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[55], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mbertopic\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BERTopic\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtopicwizard\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompatibility\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BERTopicWrapper\n\u001B[0;32m      4\u001B[0m model \u001B[38;5;241m=\u001B[39m BERTopicWrapper(BERTopic(language\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menglish\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\bertopic\\__init__.py:3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mimportlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetadata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m version\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mbertopic\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_bertopic\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BERTopic\n\u001B[0;32m      5\u001B[0m __version__ \u001B[38;5;241m=\u001B[39m version(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbertopic\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      7\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBERTopic\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m      9\u001B[0m ]\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\bertopic\\_bertopic.py:51\u001B[0m\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mbertopic\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m plotting\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mbertopic\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcluster\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BaseCluster\n\u001B[1;32m---> 51\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mbertopic\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackend\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BaseEmbedder\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mbertopic\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrepresentation\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_mmr\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m mmr\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mbertopic\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackend\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m select_backend\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\bertopic\\backend\\__init__.py:22\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# Multimodal Embeddings\u001B[39;00m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 22\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mbertopic\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackend\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_multimodal\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MultiModalBackend\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mModuleNotFoundError\u001B[39;00m:\n\u001B[0;32m     24\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`pip install bertopic[vision]` \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\bertopic\\backend\\_multimodal.py:5\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tqdm\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m List, Union\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SentenceTransformer\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mbertopic\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackend\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BaseEmbedder\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mMultiModalBackend\u001B[39;00m(BaseEmbedder):\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sentence_transformers\\__init__.py:10\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackend\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m export_dynamic_quantized_onnx_model, export_optimized_onnx_model\n\u001B[1;32m---> 10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcross_encoder\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mCrossEncoder\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CrossEncoder\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ParallelSentencesDataset, SentencesDataset\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mLoggingHandler\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LoggingHandler\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sentence_transformers\\cross_encoder\\__init__.py:3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m__future__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m annotations\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mCrossEncoder\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CrossEncoder\n\u001B[0;32m      5\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCrossEncoder\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:14\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DataLoader\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mautonotebook\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tqdm, trange\n\u001B[1;32m---> 14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, is_torch_npu_available\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtokenization_utils_base\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BatchEncoding\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PushToHubMixin\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'is_torch_npu_available' from 'transformers' (C:\\Users\\danielritter\\anaconda3\\Lib\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "execution_count": 55,
   "source": [
    "from bertopic import BERTopic\n",
    "from topicwizard.compatibility import BERTopicWrapper\n",
    "\n",
    "model = BERTopicWrapper(BERTopic(language=\"english\"))\n",
    "topicwizard.visualize(corpus=texts, model=model)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "The pyarrow installation is not built with support for the Parquet file format (DLL load failed while importing _parquet: Die angegebene Prozedur wurde nicht gefunden.)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[59], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_dataset\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BartForConditionalGeneration, BartTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments, \\\n\u001B[0;32m      3\u001B[0m     BartConfig\n\u001B[0;32m      5\u001B[0m dataset \u001B[38;5;241m=\u001B[39m load_dataset(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mc4\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124men\u001B[39m\u001B[38;5;124m\"\u001B[39m, streaming\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\datasets\\__init__.py:17\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# See the License for the specific language governing permissions and\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[0;32m     15\u001B[0m __version__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m3.0.1\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m---> 17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01marrow_dataset\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Dataset\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01marrow_reader\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ReadInstruction\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbuilder\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ArrowBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\datasets\\arrow_dataset.py:76\u001B[0m\n\u001B[0;32m     73\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcontrib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconcurrent\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m thread_map\n\u001B[0;32m     75\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m config\n\u001B[1;32m---> 76\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01marrow_reader\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ArrowReader\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01marrow_writer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ArrowWriter, OptimizedTypedSequence\n\u001B[0;32m     78\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata_files\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m sanitize_patterns\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\datasets\\arrow_reader.py:27\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TYPE_CHECKING, List, Optional, Union\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpyarrow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpa\u001B[39;00m\n\u001B[1;32m---> 27\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpyarrow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mparquet\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpq\u001B[39;00m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcontrib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconcurrent\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m thread_map\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdownload\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdownload_config\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DownloadConfig  \u001B[38;5;66;03m# noqa: F401\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\parquet\\__init__.py:20\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Licensed to the Apache Software Foundation (ASF) under one\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# or more contributor license agreements.  See the NOTICE file\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# distributed with this work for additional information\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     17\u001B[0m \n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# flake8: noqa\u001B[39;00m\n\u001B[1;32m---> 20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\parquet\\core.py:35\u001B[0m\n\u001B[0;32m     33\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpyarrow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_parquet\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01m_parquet\u001B[39;00m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[1;32m---> 35\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[0;32m     36\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe pyarrow installation is not built with support \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     37\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfor the Parquet file format (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mstr\u001B[39m(exc)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     38\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyarrow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_parquet\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (ParquetReader, Statistics,  \u001B[38;5;66;03m# noqa\u001B[39;00m\n\u001B[0;32m     41\u001B[0m                               FileMetaData, RowGroupMetaData,\n\u001B[0;32m     42\u001B[0m                               ColumnChunkMetaData,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     46\u001B[0m                               FileDecryptionProperties,\n\u001B[0;32m     47\u001B[0m                               SortingColumn)\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyarrow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfs\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (LocalFileSystem, FileSystem, FileType,\n\u001B[0;32m     49\u001B[0m                         _resolve_filesystem_and_path, _ensure_filesystem)\n",
      "\u001B[1;31mImportError\u001B[0m: The pyarrow installation is not built with support for the Parquet file format (DLL load failed while importing _parquet: Die angegebene Prozedur wurde nicht gefunden.)"
     ]
    }
   ],
   "execution_count": 59,
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments, \\\n",
    "    BartConfig\n",
    "\n",
    "dataset = load_dataset(\"c4\", \"en\", streaming=True)\n",
    "seed, buffer_size, max_length = 42, 10_000, 10_000\n",
    "train_set = dataset['train'].shuffle(seed, buffer_size=buffer_size).with_format('torch')\n",
    "val_set = dataset['validation'].shuffle(seed, buffer_size=buffer_size).take(5000).with_format('torch')\n",
    "\n",
    "tokeniser = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "def transform(data_array):\n",
    "    texts = []\n",
    "    for data in data_array:\n",
    "        texts.append(data['text'])\n",
    "\n",
    "    batch = tokeniser(texts, padding=True, truncation=True, max_length=max_length)\n",
    "    \n",
    "    with tokeniser.as_target_tokenizer():\n",
    "        labels = tokeniser(texts, padding=True, truncation=True, max_length=max_length)\n",
    "        \n",
    "    batch['labels'] = labels['input_ids']\n",
    "    \n",
    "    for k in batch:\n",
    "        batch[k] = torch.tensor(batch[k])\n",
    "    \n",
    "    return dict(batch)\n",
    "\n",
    "config = BartConfig.from_pretrained('facebook/bart-base')\n",
    "model = BartForConditionalGeneration(config)\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"checkpoints-bart-baseline-2\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5000,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=5000,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=1e-4,\n",
    "    max_steps=50_000,\n",
    "    fp16=True,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    data_collator=transform,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=val_set,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "model.eval()\n",
    "input_texts = [\n",
    "    \"Please provide a code sample that reproduces the problem you ran into.\",\n",
    "    \"It can be a Colab link or just a code snippet.\",\n",
    "    \"If you have code snippets, error messages, stack traces please provide them here as well.\",\n",
    "]\n",
    "\n",
    "inputs = tokeniser(input_texts, padding=True)\n",
    "\n",
    "input_ids = torch.tensor(inputs['input_ids']).cuda()\n",
    "model.cuda()\n",
    "output_ids = model.generate(input_ids)\n",
    "print(tokeniser.batch_decode(output_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['<tx><ld><p>Sturmtief Sabine wirbelt den Plan des Flughafen Zürich durcheinander. Ein Video eines Leser-Reporters zeigt den A380, der die Landung abbrechen muss.</p></ld><p>Der A380 der Singapore Airline musste den Landeanflug auf den Flughafen Kloten abbrechen. Beim zweiten Mal hat es geklappt wie ein Video eines Leser-Reporters zeigt.</p><p>Nicht nur der A380-Gigant musste durchstarten. Wie ein Leser-Reporter berichtet, traf es auch eine A330-300 der Swiss. Der Flieger kam von Tel Aviv.<br/><br/></p><p>(fss)</p></tx>',\n",
       "       \"<tx><ld><p>Les voisins chanteurs de la rotonde de Saint-Jean ont énervé d'autres habitants, qui ont contacté les forces de l'ordre.</p></ld><p>Inspirés par l'Italie ou l'Espagne qui semble foisonner de chanteurs donnant de la voix aux balcons, les habitants de la rotonde de Saint-Jean se sont donnés rendez-vous tous les soirs à 18h, depuis le 15 mars. D'une chanson, le concert s'est étoffé et dure désormais 15 minutes, accompagné d'instruments de musique. Certains voisins ont manifesté leur ras-le-bol et ont appelé la police, lundi, écrit la «Tribune de Genève». La chorale ne s'est pas découragée et a repris du service dès le lendemain.<br/></p><p>(lfe)</p></tx>\",\n",
       "       '<tx><ld><p>Der Kanton Aargau führt im Kampf gegen die Corona-Pandemie eine Sonderverordnung ein. Dabei soll die Polizei zur Durchsetzung der Verbote auf Echtzeit-Videoüberwachung zugreifen können.</p></ld><p>Konkret kann die Polizei zur Durchsetzung und Kontrolle der Verbote bestehende, von der Beauftragten für Öffentlichkeit und Datenschutz bewilligte optisch-elektronische Überwachungsanlagen öffentlich zugänglicher Räume zur Echtzeitüberwachung einsetzen. So steht es in der Sonderverordnung des Regierungsrats.</p><p>Gemäss Verordnung des Bundesrats sind unter anderem Menschenansammlungen von mehr als fünf Personen im öffentlichen Raum, namentlich auf öffentlichen Plätzen, auf Spazierwegen und in Parkanlagen, verboten. Bei Ansammlungen von bis zu fünf Personen sind zwischen den einzelnen Personen ein Abstand von mindestens zwei Metern einzuhalten.</p><p>Die Polizeikräfte des Kantons Aargau sind für die Durchsetzung und Kontrolle der Verbote verantwortlich. Als möglicher «Deliktsort» komme der gesamte öffentliche Raum des Kantons in Frage, heisst es in den Erläuterungen zur Sonderverordnung.</p><zt>«Virtuelle Patrouille»</zt><p>Mit den beschränkt zur Verfügung stehenden polizeilichen Kräften sei eine angemessene Kontrolle nicht umzusetzen. Der Polizei solle daher die Möglichkeit einer «virtuellen Patrouille» ermöglicht werden.</p><p>Die Polizei kann zudem ohne Bewilligung der Beauftragten für Öffentlichkeit und Datenschutz neue, zusätzliche optisch-elektronische Überwachungsanlagen zur Echtzeitüberwachung einsetzen. Diese Anlagen sind nach Aufhebung der Massnahmen des Bundesrat zu entfernen.</p><zt>Keine verdeckte Überwachung</zt><p>Wie bei bestehenden bewilligten Videoüberwachungsanlagen handle es sich nicht um eine verdeckte Überwachung, sondern um eine offene Überwachung, die präventive Zwecke erfülle und der Polizei rasche und zielgerichtete Einsätze erlaube, hält der Regierungsrat fest.</p><p>Die Sonderverordnung ist seit Donnerstag in Kraft und gilt für maximal sechs Monate. Der Regierungsrat hebt sie nach eigenen Angaben ganz oder teilweise wieder auf, sobald die Massnahmen nicht mehr nötig sind. Die Sonderverordnung schafft unter anderem auch Klarheit für die Gemeinden, die Sozialämter und für die Steuerzahlenden.<br/></p><p>(sda)</p></tx>',\n",
       "       ...,\n",
       "       '<tx><ld><p>L\\'incendie d\\'un véhicule s\\'est propagé à la forêt environnante, samedi soir. Heureusement, les pompiers ont rapidement pu le maîtriser.</p></ld><p>Samedi soir, peu après 20h30, plusieurs lecteurs signalaient un incendie sur la route de la Forclaz, au-dessus de Martigny-Combe (VS). Dépêchés sur place, les secours ont constaté qu\\'il s\\'agissait d\\'une voiture qui était sortie de route et qui avait pris feu. Ses deux occupants n\\'ont heureusement été que légèrement blessés.</p><p>L\\'incendie s\\'est propagé à la forêt environnante. Les pompiers ont pu éviter qu\\'il prenne une ampleur trop importante et ont pu le maîtriser. Les autorités ont rappelé ces derniers jours que le risque d\\'incendie de forêt était élevé en raison des faibles précipitations des dernières semaines ainsi que des températures anormalement élevées ces derniers jours. Par exemple, dans le canton de Neuchâtel, les pompiers ont déjà été mobilisés plus de 10 fois en quelques jours pour des départs de feu.</p><ka><p>19h50 - Rte de la Forclaz, 200 mètres en amont croisement Ravoire, sortie de route d\\'une voiture dans la forêt, véhicule a pris feu et embrasé les arbres alentours. Deux occupants légèrement blessés. Police cantonale et PM Martigny-Combe, CSI-A Martigny &amp; environs engagés.<a href=\"https://t.co/m9Ts3ZCFoj\">pic.twitter.com/m9Ts3ZCFoj</a>\\x96 Police Valais (@PoliceValais)<a href=\"https://twitter.com/PoliceValais/status/1249065956581785601?ref_src=twsrc%5Etfw\">April 11, 2020</a></p></ka><p>(ywe/ats)</p></tx>',\n",
       "       '<tx><ld><p>Comme l\\'épidémie de Covid-19 se poursuit, les organisateurs du concours de la chanson ont décidé de renoncer à la manifestation.</p></ld><p>La prochaine édition du concours Eurovision de la chanson, qui devait se tenir en mai à Rotterdam aux Pays-Bas, a été annulée. Une décision prise en raison de la pandémie de coronavirus, ont annoncé les organisateurs.</p><p>Plus tôt dans le mois, l\\'organisation du concours avait pourtant déclaré que son objectif était d\\'assurer le déroulement de cet évènement musical majeur malgré la crise actuelle du coronavirus.</p><p>Les Pays-Bas avaient gagné le droit d\\'organiser l\\'édition 2020 de l\\'Eurovision après la victoire du chanteur néerlandais Duncan Laurence au précédent concours, tenu en 2019 à Tel Aviv. La ville portuaire de Rotterdam avait été choisie en août pour accueillir la compétition.</p><ka><p>An official statement from the European Broadcasting Union on the<a href=\"https://twitter.com/hashtag/Eurovision?src=hash&amp;ref_src=twsrc%5Etfw\">#Eurovision</a>Song Contest 2020.<a href=\"https://t.co/b3h7akxvpF\">pic.twitter.com/b3h7akxvpF</a>\\x96 Eurovision Song Contest (@Eurovision)<a href=\"https://twitter.com/Eurovision/status/1240267936981569538?ref_src=twsrc%5Etfw\">March 18, 2020</a></p></ka><p>(nxp/ats)</p></tx>',\n",
       "       \"<tx><ld><p>Tandis que certains ont critiqué le fait de pouvoir se mêler de la politique de législature décidée par le Conseil fédéral, des sénateurs ont demandé que la crise du coronavirus soit prise en compte.</p></ld><lg><p>La socialiste tessinoise Marina Carobbio Guscetti a notamment demandé que, dans les trois prochaines années, la loi sur les épidémies soit revue.KEYSTONE/Anthony Anex</p></lg><p>Le Conseil fédéral ne doit pas oublier les conséquences de la pandémie de Covid-19 dans son programme de législature. Élaborée avant la crise, la feuille de route du gouvernement ne tient pas compte de la situation actuelle. Le Conseil des Etats y a apporté lundi quelques adaptations.</p><p>Le programme du Conseil fédéral et de l'administration pour les années 2019 à 2023 se compose de trois lignes directrices. La Suisse assure durablement sa prospérité et saisit les chances qu’offre le numérique. Elle soutient la cohésion nationale et oeuvre au renforcement de la coopération internationale. Elle assure la sécurité, s’engage pour la protection du climat et agit en partenaire fiable sur le plan international.</p><p>Ces trois lignes directrices présentées en début d'année restent importantes, même dans la situation actuelle et après l'expérience de ces trois derniers mois, a relevé la présidente de la Confédération Simonetta Sommaruga. Le Conseil fédéral a fixé 18 objectifs et 53 mesures pour les atteindre. Ils doivent être poursuivis.</p><zt>Loi sur les épidémies à revoir</zt><p>La crise du coronavirus étant survenue entre-temps, les sénateurs ont décidé de charger le Conseil fédéral d'intégrer les enseignements tirés de la pandémie de Covid-19 dans son action. Il faut une vision globale, a précisé Marina Carobbio (PS/TI) au nom de la commission. Le Conseil des Etats veut notamment que la loi sur les épidémies soit modifiée en prenant en compte le coronavirus.</p><p>Plusieurs autres précisions ont été apportées au programme gouvernemental. La protection du climat et les effets des changements climatiques font partie des grands défis de la 51e législature. Le Conseil des Etats demande une stratégie, incluant aussi la protection des ressources naturelles.</p><p>Le carnet de route accorde aussi une large place à la politique européenne. Le Conseil fédéral prévoit ainsi d'adopter le message sur l'accord institutionnel avec l'UE d'ici à la fin de la législature. Marco Chiesa (UDC/TI) aurait souhaité biffer cet objectif, prônant la voie bilatérale. Par 24 voix contre 15, le conseil ne l'a pas suivi.</p><p>Pour le reste, le Conseil fédéral est appelé à consolider les relations économiques avec le Royaume-Uni. Dans le domaine migratoire, il devra prendre des décisions concernant la reprise de plusieurs développements des acquis de Schengen et de Dublin.</p><zt>Gouvernance numérique</zt><p>Concernant la numérisation, autre grand objectif annoncé par le gouvernement, les sénateurs demandent une stratégie visant une forte gouvernance numérique. Ils préconisent aussi un accès sans entraves au digital.</p><p>La promotion de la cohésion des régions et de la compréhension entre les cultures et les communautés linguistiques est aussi essentielle, a noté Marina Carobbio. Le Conseil des Etats a décidé d'ajouter, par 26 voix contre 15, un plan d'action avec les cantons pour la promotion du plurilinguisme et de cours de langue, comme l'a proposé Charles Juillard (PDC/JU). Il n'a en revanche pas voulu d'un plan d'action contre la discrimination.</p><p>Afin de créer un environnement économique stable, le gouvernement devra encore présenter un projet permettant d'accroître le dynamisme de la place économique helvétique. Côté santé, la Suisse devrait se munir d'un système de prévention efficace.</p><zt>Ingérance critiquée</zt><p>Plusieurs sénateurs ont critiqué la procédure entourant le programme de législature et le fait que le Parlement puisse modifier le contenu de l'agenda du Conseil fédéral. Cet exercice est chronophage et contraignant, a estimé Olivier Français (PLR/VD). Il prend beaucoup de temps tant en commission qu'en plénum, a-t-il regretté.</p><p>Le Parlement devrait seulement prendre acte du document, a renchéri Damian Müller (PLR/VD). La commission spéciale qui examine le programme devrait être abolie ces prochains mois, a-t-il proposé.</p><p>«C'est un moment important pour influencer l'action future du gouvernement», a justifié Carlo Sommaruga, auteur de plusieurs propositions rejetées. Le socialiste genevois aurait notamment souhaité intégrer au programme les objectifs de développement durable de l'Agenda 2030 de l'ONU.</p><p>En 2018, la commission des institutions politiques du Conseil national avait abandonné l'idée de réformer la procédure, après un avis négatif du Conseil fédéral.</p><p>Le dossier passe au Conseil national.</p><p>(ATS)</p></tx>\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 64,
   "source": "df['content'].to_numpy()"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0, 41552, 43820,  ...,     1,     1,     1],\n",
      "        [    0, 41552, 43820,  ...,     1,     1,     1],\n",
      "        [    0, 41552, 43820,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0, 41552, 43820,  ...,     1,     1,     1],\n",
      "        [    0, 41552, 43820,  ...,     1,     1,     1],\n",
      "        [    0, 41552, 43820,  ...,  1594,  9915,     2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[    0, 41552, 43820,  ...,     1,     1,     1],\n",
      "        [    0, 41552, 43820,  ...,     1,     1,     1],\n",
      "        [    0, 41552, 43820,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0, 41552, 43820,  ...,     1,     1,     1],\n",
      "        [    0, 41552, 43820,  ...,     1,     1,     1],\n",
      "        [    0, 41552, 43820,  ...,  1594,  9915,     2]])}\n"
     ]
    }
   ],
   "execution_count": 75,
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments, BartConfig\n",
    "\n",
    "\n",
    "\n",
    "tokeniser = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "def transform_2(data_array, max_length=1000):\n",
    "    texts = []\n",
    "    for data in data_array:\n",
    "        texts.append(data)\n",
    "\n",
    "    batch = tokeniser(texts, padding=True, truncation=True, max_length=max_length)\n",
    "    \n",
    "    with tokeniser.as_target_tokenizer():\n",
    "        labels = tokeniser(texts, padding=True, truncation=True, max_length=max_length)\n",
    "        \n",
    "    batch['labels'] = labels['input_ids']\n",
    "    \n",
    "    for k in batch:\n",
    "        batch[k] = torch.tensor(batch[k])\n",
    "    \n",
    "    return dict(batch)\n",
    "\n",
    "test = transform_2(df['content'].to_numpy())\n",
    "print(test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'is_torch_npu_available' from 'transformers' (C:\\Users\\danielritter\\anaconda3\\Lib\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[80], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mbertopic\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BERTopic\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Example usage\u001B[39;00m\n\u001B[0;32m      4\u001B[0m text \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStorm Sabine throws Zurich Airport\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms plan into disarray. A video from a reader-reporter shows the A380 having to abort the landing. The Singapore Airline A380 had to abort its approach to Kloten Airport. It worked the second time, as a video from a reader-reporter shows. It wasn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt just the A380 giant that had to take off. As a reader reporter reports, a Swiss A330-300 was also hit. The plane came from Tel Aviv.\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\bertopic\\__init__.py:3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mimportlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetadata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m version\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mbertopic\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_bertopic\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BERTopic\n\u001B[0;32m      5\u001B[0m __version__ \u001B[38;5;241m=\u001B[39m version(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbertopic\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      7\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBERTopic\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m      9\u001B[0m ]\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\bertopic\\_bertopic.py:51\u001B[0m\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mbertopic\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m plotting\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mbertopic\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcluster\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BaseCluster\n\u001B[1;32m---> 51\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mbertopic\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackend\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BaseEmbedder\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mbertopic\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrepresentation\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_mmr\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m mmr\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mbertopic\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackend\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m select_backend\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\bertopic\\backend\\__init__.py:22\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# Multimodal Embeddings\u001B[39;00m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 22\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mbertopic\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackend\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_multimodal\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MultiModalBackend\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mModuleNotFoundError\u001B[39;00m:\n\u001B[0;32m     24\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`pip install bertopic[vision]` \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\bertopic\\backend\\_multimodal.py:5\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tqdm\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m List, Union\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SentenceTransformer\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mbertopic\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackend\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BaseEmbedder\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mMultiModalBackend\u001B[39;00m(BaseEmbedder):\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sentence_transformers\\__init__.py:10\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackend\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m export_dynamic_quantized_onnx_model, export_optimized_onnx_model\n\u001B[1;32m---> 10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcross_encoder\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mCrossEncoder\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CrossEncoder\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ParallelSentencesDataset, SentencesDataset\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mLoggingHandler\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LoggingHandler\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sentence_transformers\\cross_encoder\\__init__.py:3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m__future__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m annotations\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mCrossEncoder\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CrossEncoder\n\u001B[0;32m      5\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCrossEncoder\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:14\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DataLoader\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mautonotebook\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tqdm, trange\n\u001B[1;32m---> 14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, is_torch_npu_available\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtokenization_utils_base\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BatchEncoding\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PushToHubMixin\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'is_torch_npu_available' from 'transformers' (C:\\Users\\danielritter\\anaconda3\\Lib\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "execution_count": 80,
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "# Example usage\n",
    "text = [\"Storm Sabine throws Zurich Airport's plan into disarray. A video from a reader-reporter shows the A380 having to abort the landing. The Singapore Airline A380 had to abort its approach to Kloten Airport. It worked the second time, as a video from a reader-reporter shows. It wasn't just the A380 giant that had to take off. As a reader reporter reports, a Swiss A330-300 was also hit. The plane came from Tel Aviv.\"]\n",
    "\n",
    "topic_model = BERTopic()\n",
    "topics, _ = topic_model.fit_transform(text)\n",
    "print(topic_model.get_topic_info())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
