{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pipeline.service import FileService\n",
    "import urllib.request\n",
    "\n",
    "nlp_de = spacy.load('de_core_news_sm')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "cleaned_df = FileService.read_parquet_to_df(file_name='articles_cleaned', file_dir='../' + FileService.default_processed_path)\n",
    "\n",
    "texts = list(cleaned_df[cleaned_df['language'] == 'de']['content'])\n",
    "custom_german_stopwords:set = {\n",
    "    \" \", \"\\x96\", \"the\", \"to\", \"of\", \"20\", \"minuten\",\n",
    "}\n",
    "\n",
    "with urllib.request.urlopen('https://raw.githubusercontent.com/solariz/german_stopwords/refs/heads/master/german_stopwords_full.txt') as f:\n",
    "    german_stopwords_full = f.read().decode('utf-8')\n",
    "\n",
    "# Test if https://github.com/solariz/german_stopwords/blob/master/german_stopwords_full.txt helps\n",
    "# with open(os.path.normpath(\"./german_stopwords_full.txt\"), \"r\") as f:\n",
    "#    german_stopwords_full = f.readlines()\n",
    "\n",
    "stop_words = set(stopwords.words(\"german\")) | set(german_stopwords_full) | custom_german_stopwords\n",
    "\n",
    "processed_texts = []\n",
    "for idx in range(len(texts)):\n",
    "    if idx % 100 == 0:\n",
    "        print(f\"At step: {idx} of {len(texts)}\")\n",
    "    doc = texts[idx]\n",
    "    # Tokenize the document\n",
    "    doc = nlp_de(str(doc).lower())  # Lowercase and tokenize\n",
    "    tokenized_articles = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "    # Lemmatize words and remove stopwords\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokenized_articles if token not in stop_words]\n",
    "\n",
    "    processed_texts.append(lemmatized_tokens)\n",
    "\n",
    "processed_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we can optimize what we actually select... but how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some problematic characters"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T13:02:18.944498Z",
     "start_time": "2024-11-06T13:02:18.937422Z"
    }
   },
   "source": [
    "# %%script echo skipping\n",
    "import re\n",
    "\n",
    "strip_chars = \"\".join([\"«\", \"»\"])\n",
    "replace_empty = \"\".join([\"-\", \"/\", \"|\", \"#\", \".\", \"…\"])\n",
    "\n",
    "\n",
    "de_df = cleaned_df[cleaned_df['language'] == 'de']\n",
    "article_list = list(de_df['content'])\n",
    "article_list =  [re.sub(r'[«»]', '', article) for article in article_list]\n",
    "article_list =  [re.sub(r'[-/|#.…]', ' ', article) for article in article_list]\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T14:43:13.094053Z",
     "start_time": "2024-11-07T14:43:12.370213Z"
    }
   },
   "source": [
    "# %%script echo skipping\n",
    "from pipeline.service import FileService\n",
    "import spacy\n",
    "nlp_de = spacy.load('de_core_news_sm')\n",
    "\n",
    "cleaned_df = FileService.read_parquet_to_df(file_name='articles_cleaned', file_dir='../data/processed')\n",
    "de_df = cleaned_df[cleaned_df['language'] == 'de']\n",
    "article_list = list(de_df['content'])"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T14:43:16.014610Z",
     "start_time": "2024-11-07T14:43:16.011387Z"
    }
   },
   "source": [
    "# %%script echo skipping\n",
    "article_list = list(de_df['content'])\n",
    "#article_list =  [re.sub(r'[«»]', '', article) for article in article_list]\n",
    "#article_list =  [re.sub(r'[-/|#.…]', ' ', article) for article in article_list]\n",
    "\n",
    "\n",
    "articles = nlp_de.pipe(article_list, disable=[\"tagger\", \"ner\", \"textcat\"], n_process=4)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T14:43:36.743570Z",
     "start_time": "2024-11-07T14:43:22.158245Z"
    }
   },
   "source": [
    "# %%script echo skipping\n",
    "i = 0\n",
    "for article in articles:\n",
    "    i = i + 1\n",
    "    if i > 3:\n",
    "        break\n",
    "    print(article)\n",
    "    alphas = [(token, token.lemma_.lower()) for token in article if not token.is_alpha and not token.is_punct]\n",
    "    print(alphas)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Der A380 der Singapore Airline musste den Landeanflug auf den Flughafen Kloten abbrechen. Beim zweiten Mal hat es geklappt wie ein Video eines Leser Reporters zeigt. Nicht nur der A380 Gigant musste durchstarten. Wie ein Leser Reporter berichtet, traf es auch eine A330 300 der Swiss. Der Flieger kam von Tel Aviv. \n",
      "[( , ' '), (A380, 'a380'), (A380, 'a380'), (A330, 'a330'), (300, '300')]\n",
      " Konkret kann die Polizei zur Durchsetzung und Kontrolle der Verbote bestehende, von der Beauftragten für Öffentlichkeit und Datenschutz bewilligte optisch elektronische Überwachungsanlagen öffentlich zugänglicher Räume zur Echtzeitüberwachung einsetzen. So steht es in der Sonderverordnung des Regierungsrats. Gemäss Verordnung des Bundesrats sind unter anderem Menschenansammlungen von mehr als fünf Personen im öffentlichen Raum, namentlich auf öffentlichen Plätzen, auf Spazierwegen und in Parkanlagen, verboten. Bei Ansammlungen von bis zu fünf Personen sind zwischen den einzelnen Personen ein Abstand von mindestens zwei Metern einzuhalten. Die Polizeikräfte des Kantons Aargau sind für die Durchsetzung und Kontrolle der Verbote verantwortlich. Als möglicher Deliktsort komme der gesamte öffentliche Raum des Kantons in Frage, heisst es in den Erläuterungen zur Sonderverordnung. Mit den beschränkt zur Verfügung stehenden polizeilichen Kräften sei eine angemessene Kontrolle nicht umzusetzen. Der Polizei solle daher die Möglichkeit einer virtuellen Patrouille ermöglicht werden. Die Polizei kann zudem ohne Bewilligung der Beauftragten für Öffentlichkeit und Datenschutz neue, zusätzliche optisch elektronische Überwachungsanlagen zur Echtzeitüberwachung einsetzen. Diese Anlagen sind nach Aufhebung der Massnahmen des Bundesrat zu entfernen. Wie bei bestehenden bewilligten Videoüberwachungsanlagen handle es sich nicht um eine verdeckte Überwachung, sondern um eine offene Überwachung, die präventive Zwecke erfülle und der Polizei rasche und zielgerichtete Einsätze erlaube, hält der Regierungsrat fest. Die Sonderverordnung ist seit Donnerstag in Kraft und gilt für maximal sechs Monate. Der Regierungsrat hebt sie nach eigenen Angaben ganz oder teilweise wieder auf, sobald die Massnahmen nicht mehr nötig sind. Die Sonderverordnung schafft unter anderem auch Klarheit für die Gemeinden, die Sozialämter und für die Steuerzahlenden. \n",
      "[( , ' ')]\n",
      " Das Rezept war einfach: Zwei vermögende Freunde skizzierten einen luxuriösen Kombi mit zwei Türen, beauftragten den italienischen Fahrzeugbauer Intermeccanica mit der Umsetzung der Idee und liessen den exzentrischen gezeichneten Shooting Break mit einem 7 Liter V8 Motor mit mehr als ausreichender Leistung bestücken. Heraus kam ein imposantes Luxusgefährt! Die Idee, den schnellsten und luxuriösesten Kombi der Welt zu bauen, stammte von den beiden Amerikanern Charles Schwendler und Joseph Vos. Die Freunde fuhren gerne in die Berge zum Skifahren, sie ärgerten sich aber, kein sportliches Fahrzeug mit genügend Platz zu besitzen. Ihr Porsche 911 bot Fahrspass, hatte aber zu wenig Stauraum, und ihr Kombi war zwar geräumig, aber nicht sportlich. Die beiden entwarfen erste Skizzen zu einem sportlichen Kombi, der ihre Ansprüche erfüllen sollte. Sie wandten sich damit an den Kleinserien Hersteller Intermeccanica in Turin, der mit dem Italia bereits Erfahrung mit italienischem Design und amerikanischer Technik hatte, und beauftragten den Unternehmer und Inhaber Frank Reisner mit der Umsetzung des Projektes. Vorgestellt wurde der Vierplätzer an der New York Motor Show im Jahr 1969. Es wird erzählt, dass Elvis Presley gleich zwei Stück davon bestellt haben soll. Auf der Kundenliste hätten auch so illustre Namen wie Sammy Davis Jr., Frank Sinatra und andere Berühmtheiten gestanden. Motor Trend schrieb im World Automotive Yearbook 1970: Der Murena GT strahlt eine Geld spielt keine Rolle Aura aus. Er ist das perfekte Auto, um schnell über lange Wüstenstrassen zu reisen, besonders in Staaten, in denen keine Geschwindigkeitslimiten das legale Tempo einschränken An Leistung hat es nicht gemangelt. Der Motor stellte bei bei 4600 U min rund 280 PS bereit. Viel interessanter ist aber das Drehmoment von 650 Newtonmeter, die schon bei 2800 U min auf die Kurbelwelle gewuchtet wurden. Das sorgte für mehr als ausreichend Durchzugskraft in jeder Lebenslage. Wenn es sein musste, erledigte der Shooting Break den Spurt von null auf 100 km h in knapp über sieben Sekunden. Trotz des Leergewichts von knapp über 1700 Kilogramm waren dies Leistungsdaten, die sich mit Sportwagen von Ferrari und Lamborghini messen konnten. Jedenfalls dürfte die Motorisierung ausgereicht haben, um mit dem 520 cm langen und 190 cm breiten Express Kombi flott unterwegs zu sein, auch wenn er mit einem Radstand von fast drei Metern kein Kurvenkünstler gewesen sein dürfte. Der Tank fasste 95 Liter, und im Prospekt wurde mit einer Tankfüllung eine Reichweite von 725 Kilometern versprochen, was einem Benzinverbrauch von 13,1 Liter auf 100 Kilometer entsprechen würde. Für einen Motor mit sieben Litern Hubraum scheint diese Angabe reichlich optimistisch. Der in Turin produzierte Luxus GT war nur schon aufgrund seiner Grösse für Europa ungeeignet, für die amerikanischen Strassen aber ein perfektes Reiseauto. Die Innenausstattung war mit edlem Leder ausgeschlagen, und eine Klimaanlage sorgte für einen kühlen Innenraum. Die Mittelkonsole verlief bis zu den beiden hinteren Einzelsitzen und bot den Passagieren auch eine Minibar. Es wurden zehn, vielleicht auch elf Murena gebaut, danach war Schluss. Wenn einer der seltenen GT auf dem Markt auftaucht, ist die Aufregung gross. So auch auf der RM Auction Palm Beach, die derzeit stattfindet. Dort soll dieser erste je produzierte Murena unter den Hammer kommen. \n",
      "[( , ' '), (7, '7'), (V8, 'v8'), (911, '911'), (1969., '1969.'), (Jr., 'jr.'), (1970, '1970'), (4600, '4600'), (280, '280'), (650, '650'), (2800, '2800'), (100, '100'), (1700, '1700'), (520, '520'), (190, '190'), (95, '95'), (725, '725'), (13,1, '13,1'), (100, '100')]\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T14:43:48.600731Z",
     "start_time": "2024-11-07T14:43:48.584984Z"
    }
   },
   "source": [
    "# %%script echo skipping\n",
    "i = 0\n",
    "for article in articles:\n",
    "    i = i + 1\n",
    "    if i > 3:\n",
    "        break\n",
    "    print(article)\n",
    "    alphas = [(token, token.lemma_.lower()) for token in article if  not token.is_punct and not token.is_space]\n",
    "    print(alphas)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dies war das Beste, was der anscheinend völlig überforderte Restaurator …Screenshot … in Valencia zustande brachte. Die Kunst Katastrophe erinnert an das missglückte Jesus Fresko aus dem Jahr 2012.KEYSTONE Darum gehts Ein Restaurator war mit einem Auftrag völlig überfordert. Das Marienbildnis war kau mwiederzuerkennen. Es ist kein Einzelfall. Eine nach Medienberichten sehr wertvolle Kopie eines der Marienbildnisse des bedeutenden spanischen Barockmalers Bartolomé Esteban Murillo wurde in Valencia von einem mit der Ausbesserung beauftragten Restaurator bis zur Unkenntlichkeit verunstaltet. Die Nachrichtenagentur Europa Press veröffentlichte am Montag Davor und Danach Bilder, die das Ausmass der Pfuscharbeit verdeutlichen. Der Auftraggeber, ein Privatsammler, der für den Job 1200 Euro im Voraus bezahlt habe, sei aus dem Staunen nicht herausgekommen, als er das Ergebnis der völlig missglückten Arbeit gesehen habe. Der Urheber der Kunst Katastrophe erhielt den Berichten zufolge sogar eine zweite Chance, aber wieder konnte der überforderte Mann nur eine bessere Kinderzeichnung liefern. Nun solle ein anderer Experte das Bild zu retten versuchen, die Hoffnungen seien aber gering, hiess es. Der Fall weckte Erinnerungen an eine verpatzte Restaurierung eines Jesus Fresko, die 2012 weltweit für Schlagzeilen und Häme sorgte. Der Versuch einer damals 80 jährigen Amateur Restauratorin, eine Wandmalerei der Kapelle in Borja bei Saragossa aus dem 19. Jahrhundert auszubessern, endete mit der Zerstörung des Abbilds Christi. Belustigte Medien tauften seinerzeit das Bild von Ecce homo auf Ecce mono um. Aber nicht alle finden solche Verschlimmbesserungen lustig. Sie kommen leider viel häufiger als angenommen vor, klagte die Koordinatorin des spanischen Restauratoren Verbandes ACRE, María Borja Ortiz. Es gibt unzählige Fälle, bei denen Personen ohne ausreichende Ausbildung Kunstwerke für immer und ewig zerstören. Aus der Not machte die Ortschaft Borja derweil eine Tugend. Der Ecce mono wurde auch dank Marketing Aktionen zur grossen Touristen Attraktion. Das verunstaltete Bild ziert T Shirts und Tassen. Zwischen 2013 und 2018 besuchten mehr als 200 000 Touristen aus dem In  und Ausland das 4900 Seelen Dorf in Aragonien. Die Besucher zahlen sogar Eintritt, um ein Selfie mit dem Affen Jesus machen zu dürfen. \n",
      "[(Dies, 'dieser'), (war, 'sein'), (das, 'der'), (Beste, 'gute'), (was, 'wer'), (der, 'der'), (anscheinend, 'anscheinend'), (völlig, 'völlig'), (überforderte, 'überfordert'), (Restaurator, 'restaurator'), (Screenshot, 'screenshot'), (in, 'in'), (Valencia, 'valencia'), (zustande, 'zustande'), (brachte, 'bringen'), (Die, 'der'), (Kunst, 'kunst'), (Katastrophe, 'katastrophe'), (erinnert, 'erinnern'), (an, 'an'), (das, 'der'), (missglückte, 'missglückt'), (Jesus, 'jesus'), (Fresko, 'fresko'), (aus, 'aus'), (dem, 'der'), (Jahr, 'jahr'), (2012.KEYSTONE, '2012.keystone'), (Darum, 'darum'), (gehts, 'gehts'), (Ein, 'ein'), (Restaurator, 'restaurator'), (war, 'sein'), (mit, 'mit'), (einem, 'ein'), (Auftrag, 'auftrag'), (völlig, 'völlig'), (überfordert, 'überfordern'), (Das, 'der'), (Marienbildnis, 'marienbildnis'), (war, 'sein'), (kau, 'kau'), (mwiederzuerkennen, 'mwiederzuerkennen'), (Es, 'es'), (ist, 'sein'), (kein, 'kein'), (Einzelfall, 'einzelfall'), (Eine, 'ein'), (nach, 'nach'), (Medienberichten, 'medienbericht'), (sehr, 'sehr'), (wertvolle, 'wertvoll'), (Kopie, 'kopie'), (eines, 'einer'), (der, 'der'), (Marienbildnisse, 'marienbildnis'), (des, 'der'), (bedeutenden, 'bedeutend'), (spanischen, 'spanisch'), (Barockmalers, 'barockmaler'), (Bartolomé, 'bartolomé'), (Esteban, 'esteban'), (Murillo, 'murillo'), (wurde, 'werden'), (in, 'in'), (Valencia, 'valencia'), (von, 'von'), (einem, 'ein'), (mit, 'mit'), (der, 'der'), (Ausbesserung, 'ausbesserung'), (beauftragten, 'beauftragt'), (Restaurator, 'restaurator'), (bis, 'bis'), (zur, 'zu'), (Unkenntlichkeit, 'unkenntlichkeit'), (verunstaltet, 'verunstalten'), (Die, 'der'), (Nachrichtenagentur, 'nachrichtenagentur'), (Europa, 'europa'), (Press, 'press'), (veröffentlichte, 'veröffentlichen'), (am, 'an'), (Montag, 'montag'), (Davor, 'davor'), (und, 'und'), (Danach, 'danach'), (Bilder, 'bild'), (die, 'der'), (das, 'der'), (Ausmass, 'ausmass'), (der, 'der'), (Pfuscharbeit, 'pfuscharbeit'), (verdeutlichen, 'verdeutlichen'), (Der, 'der'), (Auftraggeber, 'auftraggeber'), (ein, 'ein'), (Privatsammler, 'privatsammler'), (der, 'der'), (für, 'für'), (den, 'der'), (Job, 'job'), (1200, '1200'), (Euro, 'euro'), (im, 'in'), (Voraus, 'voraus'), (bezahlt, 'bezahlen'), (habe, 'haben'), (sei, 'sein'), (aus, 'aus'), (dem, 'der'), (Staunen, 'staun'), (nicht, 'nicht'), (herausgekommen, 'herausgekommen'), (als, 'als'), (er, 'er'), (das, 'der'), (Ergebnis, 'ergebnis'), (der, 'der'), (völlig, 'völlig'), (missglückten, 'missglückt'), (Arbeit, 'arbeit'), (gesehen, 'sehen'), (habe, 'haben'), (Der, 'der'), (Urheber, 'urheber'), (der, 'der'), (Kunst, 'kunst'), (Katastrophe, 'katastrophe'), (erhielt, 'erhalten'), (den, 'der'), (Berichten, 'bericht'), (zufolge, 'zufolge'), (sogar, 'sogar'), (eine, 'ein'), (zweite, 'zweiter'), (Chance, 'chance'), (aber, 'aber'), (wieder, 'wieder'), (konnte, 'können'), (der, 'der'), (überforderte, 'überfordert'), (Mann, 'mann'), (nur, 'nur'), (eine, 'ein'), (bessere, 'gut'), (Kinderzeichnung, 'kinderzeichnung'), (liefern, 'liefern'), (Nun, 'nun'), (solle, 'sollen'), (ein, 'ein'), (anderer, 'anderer'), (Experte, 'experte'), (das, 'der'), (Bild, 'bild'), (zu, 'zu'), (retten, 'retten'), (versuchen, 'versuchen'), (die, 'der'), (Hoffnungen, 'hoffnung'), (seien, 'sein'), (aber, 'aber'), (gering, 'gering'), (hiess, 'hiess'), (es, 'es'), (Der, 'der'), (Fall, 'fall'), (weckte, 'wecken'), (Erinnerungen, 'erinnerung'), (an, 'an'), (eine, 'ein'), (verpatzte, 'verpatzt'), (Restaurierung, 'restaurierung'), (eines, 'ein'), (Jesus, 'jesus'), (Fresko, 'fresko'), (die, 'der'), (2012, '2012'), (weltweit, 'weltweit'), (für, 'für'), (Schlagzeilen, 'schlagzeile'), (und, 'und'), (Häme, 'häme'), (sorgte, 'sorgen'), (Der, 'der'), (Versuch, 'versuch'), (einer, 'ein'), (damals, 'damals'), (80, '80'), (jährigen, 'jährig'), (Amateur, 'amateur'), (Restauratorin, 'restauratorin'), (eine, 'ein'), (Wandmalerei, 'wandmalerei'), (der, 'der'), (Kapelle, 'kapelle'), (in, 'in'), (Borja, 'borja'), (bei, 'bei'), (Saragossa, 'saragossa'), (aus, 'aus'), (dem, 'der'), (19., '19.'), (Jahrhundert, 'jahrhundert'), (auszubessern, 'ausbessern'), (endete, 'enden'), (mit, 'mit'), (der, 'der'), (Zerstörung, 'zerstörung'), (des, 'der'), (Abbilds, 'abbild'), (Christi, 'christi'), (Belustigte, 'belustigte'), (Medien, 'medium'), (tauften, 'taufen'), (seinerzeit, 'seinerzeit'), (das, 'der'), (Bild, 'bild'), (von, 'von'), (Ecce, 'ecce'), (homo, 'homo'), (auf, 'auf'), (Ecce, 'ecce'), (mono, 'mono'), (um, 'um'), (Aber, 'aber'), (nicht, 'nicht'), (alle, 'alle'), (finden, 'finden'), (solche, 'solcher'), (Verschlimmbesserungen, 'verschlimmbesserung'), (lustig, 'lustig'), (Sie, 'sie'), (kommen, 'kommen'), (leider, 'leider'), (viel, 'viel'), (häufiger, 'häufig'), (als, 'als'), (angenommen, 'annehmen'), (vor, 'vor'), (klagte, 'klagen'), (die, 'der'), (Koordinatorin, 'koordinatorin'), (des, 'der'), (spanischen, 'spanisch'), (Restauratoren, 'restaurator'), (Verbandes, 'verbandes'), (ACRE, 'acre'), (María, 'maría'), (Borja, 'borja'), (Ortiz, 'ortiz'), (Es, 'es'), (gibt, 'geben'), (unzählige, 'unzählig'), (Fälle, 'fall'), (bei, 'bei'), (denen, 'der'), (Personen, 'person'), (ohne, 'ohne'), (ausreichende, 'ausreichend'), (Ausbildung, 'ausbildung'), (Kunstwerke, 'kunstwerk'), (für, 'für'), (immer, 'immer'), (und, 'und'), (ewig, 'ewig'), (zerstören, 'zerstören'), (Aus, 'aus'), (der, 'der'), (Not, 'not'), (machte, 'machen'), (die, 'der'), (Ortschaft, 'ortschaft'), (Borja, 'borja'), (derweil, 'derweil'), (eine, 'ein'), (Tugend, 'tugend'), (Der, 'der'), (Ecce, 'ecce'), (mono, 'mono'), (wurde, 'werden'), (auch, 'auch'), (dank, 'dank'), (Marketing, 'marketing'), (Aktionen, 'aktion'), (zur, 'zu'), (grossen, 'gross'), (Touristen, 'tourist'), (Attraktion, 'attraktion'), (Das, 'der'), (verunstaltete, 'verunstaltet'), (Bild, 'bild'), (ziert, 'zieren'), (T, 't'), (Shirts, 'shirts'), (und, 'und'), (Tassen, 'tasse'), (Zwischen, 'zwischen'), (2013, '2013'), (und, 'und'), (2018, '2018'), (besuchten, 'besuchen'), (mehr, 'mehr'), (als, 'als'), (200, '200'), (000, '000'), (Touristen, 'tourist'), (aus, 'aus'), (dem, 'der'), (In, 'in'), (und, 'und'), (Ausland, 'ausland'), (das, 'der'), (4900, '4900'), (Seelen, 'seele'), (Dorf, 'dorf'), (in, 'in'), (Aragonien, 'aragonien'), (Die, 'der'), (Besucher, 'besucher'), (zahlen, 'zahlen'), (sogar, 'sogar'), (Eintritt, 'eintritt'), (um, 'um'), (ein, 'ein'), (Selfie, 'selfie'), (mit, 'mit'), (dem, 'der'), (Affen, 'affe'), (Jesus, 'jesus'), (machen, 'machen'), (zu, 'zu'), (dürfen, 'dürfen')]\n",
      " Der Unfall ereignete sich kurz vor 0.30 Uhr am Montag auf der Strecke zwischen Büron LU und Schmiedrued AG im Gebiet Rehhag. Ein 21 jähriger Schweizer fuhr mit nicht angepasster Geschwindigkeit, wie die Kantonspolizei Aargau berichtete. Zudem wurden dem Fahrer die Strassenverhältnisse zum Verhängnis. In einer Kurve kam er mit seinem Opel Corsa ins Schleudern und fuhr gegen ein Strassenbord, worauf sich das Auto mehrmals überschlug. Bei dem Unfall wurde der Fahrer leicht verletzt und mit der Ambulanz ins Spital gefahren. Am Auto entstand grosser Sachschaden, wie die Kapo Aargau mitteilt. \n",
      "[(Der, 'der'), (Unfall, 'unfall'), (ereignete, 'ereignen'), (sich, 'sich'), (kurz, 'kurz'), (vor, 'vor'), (0.30, '0.30'), (Uhr, 'uhr'), (am, 'an'), (Montag, 'montag'), (auf, 'auf'), (der, 'der'), (Strecke, 'strecke'), (zwischen, 'zwischen'), (Büron, 'büron'), (LU, 'lu'), (und, 'und'), (Schmiedrued, 'schmiedrued'), (AG, 'ag'), (im, 'in'), (Gebiet, 'gebiet'), (Rehhag, 'rehhag'), (Ein, 'ein'), (21, '21'), (jähriger, 'jährig'), (Schweizer, 'schweizer'), (fuhr, 'fahren'), (mit, 'mit'), (nicht, 'nicht'), (angepasster, 'angepasst'), (Geschwindigkeit, 'geschwindigkeit'), (wie, 'wie'), (die, 'der'), (Kantonspolizei, 'kantonspolizei'), (Aargau, 'aargau'), (berichtete, 'berichten'), (Zudem, 'zudem'), (wurden, 'werden'), (dem, 'der'), (Fahrer, 'fahrer'), (die, 'der'), (Strassenverhältnisse, 'strassenverhältnisse'), (zum, 'zu'), (Verhängnis, 'verhängnis'), (In, 'in'), (einer, 'ein'), (Kurve, 'kurve'), (kam, 'kommen'), (er, 'er'), (mit, 'mit'), (seinem, 'sein'), (Opel, 'opel'), (Corsa, 'corsa'), (ins, 'in'), (Schleudern, 'schleudern'), (und, 'und'), (fuhr, 'fahren'), (gegen, 'gegen'), (ein, 'ein'), (Strassenbord, 'strassenbord'), (worauf, 'worauf'), (sich, 'sich'), (das, 'der'), (Auto, 'auto'), (mehrmals, 'mehrmals'), (überschlug, 'überschlagen'), (Bei, 'bei'), (dem, 'der'), (Unfall, 'unfall'), (wurde, 'werden'), (der, 'der'), (Fahrer, 'fahrer'), (leicht, 'leicht'), (verletzt, 'verletzen'), (und, 'und'), (mit, 'mit'), (der, 'der'), (Ambulanz, 'ambulanz'), (ins, 'in'), (Spital, 'spital'), (gefahren, 'fahren'), (Am, 'an'), (Auto, 'auto'), (entstand, 'entstehen'), (grosser, 'gross'), (Sachschaden, 'sachschaden'), (wie, 'wie'), (die, 'der'), (Kapo, 'kapo'), (Aargau, 'aargau'), (mitteilt, 'mitteilen')]\n",
      " Diese Woche haben die Schweizer Rapper Stress und Nemo das Musikvideo zur gemeinsamen Single Legend gedroppt. Der Star des Clips: Ein Rössli, mit dem sie anstelle eines protzigen Sportwagens an einer Tankstelle posieren. Ob sie sich dabei von Lil Nas X inspirieren liessen, der mit seiner Pferde Hymne Old Town Road wohl den grössten Hit von 2019 landete? Ebenso gut könnte die Inspiration aber auch aus der hiesigen Musikszene stammen: Schweizer Rapper setzen in ihren Musikvideos nämlich auffällig oft auf Pferd statt auf Porsche . \n",
      "[(Diese, 'dieser'), (Woche, 'woche'), (haben, 'haben'), (die, 'der'), (Schweizer, 'schweizer'), (Rapper, 'rapper'), (Stress, 'stress'), (und, 'und'), (Nemo, 'nemo'), (das, 'der'), (Musikvideo, 'musikvideo'), (zur, 'zu'), (gemeinsamen, 'gemeinsam'), (Single, 'single'), (Legend, 'legend'), (gedroppt, 'droppen'), (Der, 'der'), (Star, 'star'), (des, 'der'), (Clips, 'clip'), (Ein, 'ein'), (Rössli, 'rössli'), (mit, 'mit'), (dem, 'der'), (sie, 'sie'), (anstelle, 'anstelle'), (eines, 'ein'), (protzigen, 'protzig'), (Sportwagens, 'sportwagen'), (an, 'an'), (einer, 'ein'), (Tankstelle, 'tankstelle'), (posieren, 'posieren'), (Ob, 'ob'), (sie, 'sie'), (sich, 'sich'), (dabei, 'dabei'), (von, 'von'), (Lil, 'lil'), (Nas, 'nas'), (X, 'x'), (inspirieren, 'inspirieren'), (liessen, 'liessen'), (der, 'der'), (mit, 'mit'), (seiner, 'sein'), (Pferde, 'pferde'), (Hymne, 'hymne'), (Old, 'old'), (Town, 'town'), (Road, 'road'), (wohl, 'wohl'), (den, 'der'), (grössten, 'grösst'), (Hit, 'hit'), (von, 'von'), (2019, '2019'), (landete, 'landen'), (Ebenso, 'ebenso'), (gut, 'gut'), (könnte, 'können'), (die, 'der'), (Inspiration, 'inspiration'), (aber, 'aber'), (auch, 'auch'), (aus, 'aus'), (der, 'der'), (hiesigen, 'hiesig'), (Musikszene, 'musikszene'), (stammen, 'stammen'), (Schweizer, 'schweizer'), (Rapper, 'rapper'), (setzen, 'setzen'), (in, 'in'), (ihren, 'ihr'), (Musikvideos, 'musikvideo'), (nämlich, 'nämlich'), (auffällig, 'auffällig'), (oft, 'oft'), (auf, 'auf'), (Pferd, 'pferd'), (statt, 'statt'), (auf, 'auf'), (Porsche, 'porsche')]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual selection of: https://universaldependencies.org/u/pos/\n",
    "\n",
    "# Dictionaries with some POS removed\n",
    "Dataset 1 - Throw away those:\n",
    "* \"ADP\", adapositon\n",
    "* \"ADV\", adverb\n",
    "* \"AUX\", auxiliary\n",
    "* \"CCONJ\", coordinating conunction\n",
    "* \"DET\", determiner\n",
    "* \"INTJ\", interjection\n",
    "* \"NUM\", numeral\n",
    "* \"PART\", particle\n",
    "* \"PRON\", pronoun\n",
    "* \"PUNCT\", punctuation\n",
    "* \"SCONJ\", subordinating conjunction\n",
    "* \"SYM\" symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think it might make mostly sense to keep these double words?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T14:44:45.292969Z",
     "start_time": "2024-11-07T14:44:44.571107Z"
    }
   },
   "source": [
    "import spacy\n",
    "import os\n",
    "from gensim import corpora\n",
    "from pipeline.service import FileService\n",
    "\n",
    "stopwords = {\"#\", \"*\", \"--\"}\n",
    "\n",
    "cleaned_df = FileService.read_parquet_to_df(file_name='articles_cleaned', file_dir='../data/processed')\n",
    "de_df = cleaned_df[cleaned_df['language'] == 'de']\n",
    "\n",
    "nlp_de = spacy.load('de_core_news_sm')\n",
    "nlp_de.Defaults.stop_words |= stopwords\n"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T14:49:13.836632Z",
     "start_time": "2024-11-07T14:46:47.875444Z"
    }
   },
   "source": [
    "articles = nlp_de.pipe(de_df['content'], disable=[\"tagger\", \"ner\", \"textcat\"], n_process=4)\n",
    "\n",
    "# Tags to be removed: https://universaldependencies.org/u/pos/\n",
    "pos_to_remove= [\"ADP\", \"ADV\", \"AUX\", \"CCONJ\", \"DET\", \"INTJ\", \"NUM\", \"PART\", \"PRON\", \"PUNCT\", \"SCONJ\", \"SYM\"]\n",
    "\n",
    "tokenized_articles = []\n",
    "i = 0\n",
    "for article in articles:\n",
    "   i += 1\n",
    "   if i % 1000 == 0:\n",
    "      print(f\"At step: {i} of {len(de_df)}\")\n",
    "\n",
    "   article_tokens = []\n",
    "   for token in article:\n",
    "      if (\n",
    "         token.pos_ not in pos_to_remove # Remove defined parts of speech\n",
    "         and not token.is_stop # Token is not a stopword\n",
    "         and not token.is_space\n",
    "      ):\n",
    "         article_tokens.append(token.lemma_.lower())\n",
    "\n",
    "   tokenized_articles.append(article_tokens)\n",
    "\n",
    "dictionary_german_removed_pos = corpora.Dictionary(tokenized_articles)\n",
    "dictionary_german_removed_pos.save(fname_or_handle=os.path.normpath(\"../models/dictionaries/dictionary-german-removed-pos\"))\n",
    "print(\"Exported dictionary: dictionary_german_removed_pos\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At step: 1000 of 8044\n",
      "At step: 2000 of 8044\n",
      "At step: 3000 of 8044\n",
      "At step: 4000 of 8044\n",
      "At step: 5000 of 8044\n",
      "At step: 6000 of 8044\n",
      "At step: 7000 of 8044\n",
      "At step: 8000 of 8044\n",
      "Exported dictionary: dictionary_german_removed_pos\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary_german_removed_pos.most_common(n=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionaries with only nouns\n",
    "Inspired by https://aclanthology.org/U15-1013"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T14:53:44.253183Z",
     "start_time": "2024-11-07T14:51:24.462386Z"
    }
   },
   "source": [
    "articles = nlp_de.pipe(de_df['content'], disable=[\"tagger\", \"ner\", \"textcat\"], n_process=4)\n",
    "\n",
    "# The only tag to keep is nouns: https://universaldependencies.org/u/pos/\n",
    "nouns = [\"NOUN\", \"PROPN\"]\n",
    "\n",
    "tokenized_articles = []\n",
    "i = 0\n",
    "for article in articles:\n",
    "   i += 1\n",
    "   if i % 1000 == 0:\n",
    "      print(f\"At step: {i} of {len(de_df)}\")\n",
    "\n",
    "   article_tokens = []\n",
    "   for token in article:\n",
    "      if (\n",
    "         token.pos_ in nouns\n",
    "         and not token.is_stop\n",
    "         ):\n",
    "         article_tokens.append(token.lemma_.lower())\n",
    "\n",
    "   tokenized_articles.append(article_tokens)\n",
    "\n",
    "dictionary_german_noun_only = corpora.Dictionary(tokenized_articles)\n",
    "dictionary_german_noun_only.save(fname_or_handle=os.path.normpath(\"../models/dictionaries/dictionary-german-noun-only\"))\n",
    "print(\"Exported dictionary: dictionary_german_noun-only\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At step: 1000 of 8044\n",
      "At step: 2000 of 8044\n",
      "At step: 3000 of 8044\n",
      "At step: 4000 of 8044\n",
      "At step: 5000 of 8044\n",
      "At step: 6000 of 8044\n",
      "At step: 7000 of 8044\n",
      "At step: 8000 of 8044\n",
      "Exported dictionary: dictionary_german_noun-only\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very few weird words in top 300:\n",
    "\n",
    "```py\n",
    "stopwords = [\"\\x96\", \"the\", \"#\", \"keystone\", \"*\", \"--\", \"a\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary_german_noun_only.most_common(n=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionaries with some POS removed"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T09:26:35.096276Z",
     "start_time": "2024-11-08T09:26:32.745614Z"
    }
   },
   "source": [
    "import spacy\n",
    "import os\n",
    "from gensim import corpora\n",
    "from pipeline.service import FileService\n",
    "\n",
    "\n",
    "cleaned_df = FileService.read_parquet_to_df(file_name='articles_cleaned', file_dir='../data/processed')\n",
    "fr_df = cleaned_df[cleaned_df['language'] == 'fr']\n",
    "\n",
    "nlp_fr = spacy.load('fr_core_news_sm')\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T15:03:33.670522Z",
     "start_time": "2024-11-07T14:58:38.493942Z"
    }
   },
   "source": [
    "articles = nlp_fr.pipe(fr_df['content'], disable=[\"tagger\", \"ner\", \"textcat\"], n_process=4)\n",
    "\n",
    "# Tags to be removed: https://universaldependencies.org/u/pos/\n",
    "pos_to_remove= [\"ADP\", \"ADV\", \"AUX\", \"CCONJ\", \"DET\", \"INTJ\", \"NUM\", \"PART\", \"PRON\", \"PUNCT\", \"SCONJ\", \"SYM\", ]\n",
    "stopwords = [\"«\", \"-\", \" \", \"m.\", \"#\", \"–\", \"\\x96\", \"*\", \"c\\x92est\", \"d\\x92un\", \"-t\", \"/\" ,\n",
    "             \"qu\\x92il\", \"webtv@20minutes.ch\", \"j.\", \"d\\x92autre\", \"https://t.co\", \"c\\x9cur\", \n",
    "             \"j\\x92ai\", \"h.\", \"o\", \"n\\x92er\", \"n\\x92a\", \"c.\", \"s.\", \".keystone\", \"n\\x92y\", \n",
    "             \"s\\x9cur\", \"l.\", \"b.\", \"\\x9cuvre\", \"jusqu\\x92à\", \"n\\x92aver\", \"|\", \"''\", \"n\\x92est\", \"…\"] # And many more...\n",
    "tokenized_articles = []\n",
    "i = 0\n",
    "len_df = len(fr_df)\n",
    "for article in articles:\n",
    "   i += 1\n",
    "   if i % 1000 == 0:\n",
    "      print(f\"At step: {i} of {len_df}\")\n",
    "\n",
    "   article_tokens = []\n",
    "   for token in article:\n",
    "      if (\n",
    "         token.pos_ not in pos_to_remove # Remove defined parts of speech\n",
    "         and not token.is_stop # Token is not a stopword\n",
    "         and not token.is_space\n",
    "      ):\n",
    "         article_tokens.append(token.lemma_.lower())\n",
    "\n",
    "   tokenized_articles.append(article_tokens)\n",
    "\n",
    "dictionary_french_removed_pos = corpora.Dictionary(tokenized_articles)\n",
    "dictionary_french_removed_pos.save(fname_or_handle=os.path.normpath(\"../models/dictionaries/dictionary-french-removed-pos\"))\n",
    "print(\"Exported dictionary: dictionary_french_removed_pos\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At step: 1000 of 15511\n",
      "At step: 2000 of 15511\n",
      "At step: 3000 of 15511\n",
      "At step: 4000 of 15511\n",
      "At step: 5000 of 15511\n",
      "At step: 6000 of 15511\n",
      "At step: 7000 of 15511\n",
      "At step: 8000 of 15511\n",
      "At step: 9000 of 15511\n",
      "At step: 10000 of 15511\n",
      "At step: 11000 of 15511\n",
      "At step: 12000 of 15511\n",
      "At step: 13000 of 15511\n",
      "At step: 14000 of 15511\n",
      "At step: 15000 of 15511\n",
      "Exported dictionary: dictionary_french_removed_pos\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still contains some \n",
    "* special characters\n",
    "* abbreviations \"a.\", \"l.\" -> might this be the end of a sentence? \n",
    "* a lot of escaped \"\\x92\"... this might be apostrophes -> encoding issue during preprocessing?\n",
    "\n",
    "```py\n",
    "stopwords = [\"«\", \"-\", \" \", \"m.\", \"#\", \"–\", \"\\x96\", \"*\", \"c\\x92est\", \"d\\x92un\", \"-t\", \"/\" ,\n",
    "\"qu\\x92il\", \"webtv@20minutes.ch\", \"j.\", \"d\\x92autre\", \"https://t.co\", \"c\\x9cur\", \n",
    "\"j\\x92ai\", \"h.\", \"o\", \"n\\x92er\", \"n\\x92a\", \"c.\", \"s.\", \".keystone\", \"n\\x92y\", \n",
    "\"s\\x9cur\", \"l.\", \"b.\", \"\\x9cuvre\", \"jusqu\\x92à\", \"n\\x92aver\", \"|\", \"''\", \"n\\x92est\", \"…\"] # And many more...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary_french_removed_pos.most_common(n=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionaries with only nouns\n",
    "Inspired by https://aclanthology.org/U15-1013"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T09:36:14.614522Z",
     "start_time": "2024-11-08T09:26:46.536398Z"
    }
   },
   "source": [
    "articles = nlp_fr.pipe(fr_df['content'], disable=[\"tagger\", \"ner\", \"textcat\"], n_process=4)\n",
    "\n",
    "# The only tag to keep is nouns: https://universaldependencies.org/u/pos/\n",
    "nouns = [\"NOUN\", \"PROPN\"]\n",
    "stopwords = [\"«\", \"-\", \"l’\", \"afp\", \"m.\", \"a\", \"#\"] # Top 300\n",
    "tokenized_articles = []\n",
    "i = 0\n",
    "len_df = len(fr_df)\n",
    "for article in articles:\n",
    "   i += 1\n",
    "   if i % 1000 == 0:\n",
    "      print(f\"At step: {i} of {len_df}\")\n",
    "\n",
    "   article_tokens = []\n",
    "   for token in article:\n",
    "      if (\n",
    "         token.pos_ in nouns\n",
    "         and not token.is_stop\n",
    "         ):\n",
    "         article_tokens.append(token.lemma_.lower())\n",
    "\n",
    "   tokenized_articles.append(article_tokens)\n",
    "\n",
    "dictionary_french_noun_only = corpora.Dictionary(tokenized_articles)\n",
    "dictionary_french_noun_only.save(fname_or_handle=os.path.normpath(\"../models/dictionaries/dictionary-french-noun-only\"))\n",
    "print(\"Exported dictionary: dictionary_french_noun-only\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At step: 1000 of 15511\n",
      "At step: 2000 of 15511\n",
      "At step: 3000 of 15511\n",
      "At step: 4000 of 15511\n",
      "At step: 5000 of 15511\n",
      "At step: 6000 of 15511\n",
      "At step: 7000 of 15511\n",
      "At step: 8000 of 15511\n",
      "At step: 9000 of 15511\n",
      "At step: 10000 of 15511\n",
      "At step: 11000 of 15511\n",
      "At step: 12000 of 15511\n",
      "At step: 13000 of 15511\n",
      "At step: 14000 of 15511\n",
      "At step: 15000 of 15511\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[0;32m~/git/uzh/20-minuten/.venv/lib/python3.11/site-packages/gensim/utils.py:763\u001B[0m, in \u001B[0;36mSaveLoad.save\u001B[0;34m(self, fname_or_handle, separately, sep_limit, ignore, pickle_protocol)\u001B[0m\n\u001B[1;32m    762\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 763\u001B[0m     \u001B[43m_pickle\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdump\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfname_or_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprotocol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpickle_protocol\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    764\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msaved \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m object\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n",
      "\u001B[0;31mTypeError\u001B[0m: file must have a 'write' attribute",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 25\u001B[0m\n\u001B[1;32m     22\u001B[0m    tokenized_articles\u001B[38;5;241m.\u001B[39mappend(article_tokens)\n\u001B[1;32m     24\u001B[0m dictionary_french_noun_only \u001B[38;5;241m=\u001B[39m corpora\u001B[38;5;241m.\u001B[39mDictionary(tokenized_articles)\n\u001B[0;32m---> 25\u001B[0m \u001B[43mdictionary_french_noun_only\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfname_or_handle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnormpath\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m../models/dictionaries/dictionary-french-noun-only\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExported dictionary: dictionary_french_noun-only\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/git/uzh/20-minuten/.venv/lib/python3.11/site-packages/gensim/utils.py:766\u001B[0m, in \u001B[0;36mSaveLoad.save\u001B[0;34m(self, fname_or_handle, separately, sep_limit, ignore, pickle_protocol)\u001B[0m\n\u001B[1;32m    764\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msaved \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m object\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m    765\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:  \u001B[38;5;66;03m# `fname_or_handle` does not have write attribute\u001B[39;00m\n\u001B[0;32m--> 766\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_smart_save\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfname_or_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseparately\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msep_limit\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpickle_protocol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpickle_protocol\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/git/uzh/20-minuten/.venv/lib/python3.11/site-packages/gensim/utils.py:575\u001B[0m, in \u001B[0;36mSaveLoad._smart_save\u001B[0;34m(self, fname, separately, sep_limit, ignore, pickle_protocol)\u001B[0m\n\u001B[1;32m    572\u001B[0m     compress, suffix \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnpz\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m fname\u001B[38;5;241m.\u001B[39mendswith(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.gz\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m fname\u001B[38;5;241m.\u001B[39mendswith(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.bz2\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m (\u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnpy\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    573\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m compress, \u001B[38;5;28;01mlambda\u001B[39;00m \u001B[38;5;241m*\u001B[39margs: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(args \u001B[38;5;241m+\u001B[39m (suffix,))\n\u001B[0;32m--> 575\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_smart_save\u001B[39m(\n\u001B[1;32m    576\u001B[0m         \u001B[38;5;28mself\u001B[39m, fname,\n\u001B[1;32m    577\u001B[0m         separately\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, sep_limit\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1024\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m2\u001B[39m, ignore\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mfrozenset\u001B[39m(), pickle_protocol\u001B[38;5;241m=\u001B[39mPICKLE_PROTOCOL,\n\u001B[1;32m    578\u001B[0m     ):\n\u001B[1;32m    579\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Save the object to a file. Used internally by :meth:`gensim.utils.SaveLoad.save()`.\u001B[39;00m\n\u001B[1;32m    580\u001B[0m \n\u001B[1;32m    581\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    602\u001B[0m \n\u001B[1;32m    603\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m    604\u001B[0m     compress, subname \u001B[38;5;241m=\u001B[39m SaveLoad\u001B[38;5;241m.\u001B[39m_adapt_by_suffix(fname)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very few weird words in top 300:\n",
    "```py\n",
    "stopwords = [\"«\", \"-\", \"l’\", \"afp\", \"m.\", \"a\", \"#\"] # Top 300\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary_french_noun_only.most_common(n=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
