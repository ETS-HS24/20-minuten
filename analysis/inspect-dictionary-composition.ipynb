{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "import spacy\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pipeline.service import FileService\n",
    "\n",
    "nlp_de = spacy.load('de_core_news_sm')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "cleaned_df = FileService.read_parquet_to_df(file_name='articles_cleaned')\n",
    "\n",
    "texts = list(cleaned_df[cleaned_df['language'] == 'de']['content'])\n",
    "custom_german_stopwords:set = {\n",
    "    \" \", \"\\x96\", \"the\", \"to\", \"of\", \"20\", \"minuten\",\n",
    "}\n",
    "\n",
    "# Test if https://github.com/solariz/german_stopwords/blob/master/german_stopwords_full.txt helps\n",
    "with open(os.path.normpath(\"./german_stopwords_full.txt\"), \"r\") as f:\n",
    "    german_stopwords_full = f.readlines()\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words(\"german\")) | set(german_stopwords_full) | custom_german_stopwords\n",
    "\n",
    "\n",
    "processed_texts = []\n",
    "for idx in range(len(texts)):\n",
    "    if idx % 100 == 0:\n",
    "        print(f\"At step: {idx} of {len(texts)}\")\n",
    "    doc = texts[idx]\n",
    "    # Tokenize the document\n",
    "    doc = nlp(str(doc).lower())  # Lowercase and tokenize\n",
    "    tokenized_articles = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "    # Lemmatize words and remove stopwords\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokenized_articles if token not in stop_words]\n",
    "\n",
    "    processed_texts.append(lemmatized_tokens)\n",
    "\n",
    "processed_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we can optimize what we actually select... but how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some problematic characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "import re\n",
    "\n",
    "strip_chars = \"\".join([\"«\", \"»\"])\n",
    "replace_empty = \"\".join([\"-\", \"/\", \"|\", \"#\", \".\", \"…\"])\n",
    "\n",
    "\n",
    "de_df = cleaned_df[cleaned_df['language'] == 'de']\n",
    "article_list = list(de_df['content'])\n",
    "article_list =  [re.sub(r'[«»]', '', article) for article in article_list]\n",
    "article_list =  [re.sub(r'[-/|#.…]', ' ', article) for article in article_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "from pipeline.service import FileService\n",
    "import spacy\n",
    "nlp_de = spacy.load('de_core_news_sm')\n",
    "\n",
    "cleaned_df = FileService.read_parquet_to_df(file_name='articles_cleaned')\n",
    "de_df = cleaned_df[cleaned_df['language'] == 'de']\n",
    "article_list = list(de_df['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "article_list = list(de_df['content'])\n",
    "#article_list =  [re.sub(r'[«»]', '', article) for article in article_list]\n",
    "#article_list =  [re.sub(r'[-/|#.…]', ' ', article) for article in article_list]\n",
    "\n",
    "\n",
    "articles = nlp_de.pipe(article_list, disable=[\"tagger\", \"ner\", \"textcat\"], n_process=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "i = 0\n",
    "for article in articles:\n",
    "    i = i + 1\n",
    "    if i > 3:\n",
    "        break\n",
    "    print(article)\n",
    "    alphas = [(token, token.lemma_.lower()) for token in article if not token.is_alpha and not token.is_punct]\n",
    "    print(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "i = 0\n",
    "for article in articles:\n",
    "    i = i + 1\n",
    "    if i > 3:\n",
    "        break\n",
    "    print(article)\n",
    "    alphas = [(token, token.lemma_.lower()) for token in article if  not token.is_punct and not token.is_space]\n",
    "    print(alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual selection of: https://universaldependencies.org/u/pos/\n",
    "\n",
    "# Dictionaries with some POS removed\n",
    "Dataset 1 - Throw away those:\n",
    "* \"ADP\", adapositon\n",
    "* \"ADV\", adverb\n",
    "* \"AUX\", auxiliary\n",
    "* \"CCONJ\", coordinating conunction\n",
    "* \"DET\", determiner\n",
    "* \"INTJ\", interjection\n",
    "* \"NUM\", numeral\n",
    "* \"PART\", particle\n",
    "* \"PRON\", pronoun\n",
    "* \"PUNCT\", punctuation\n",
    "* \"SCONJ\", subordinating conjunction\n",
    "* \"SYM\" symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think it might make mostly sense to keep these double words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "from gensim import corpora\n",
    "from pipeline.service import FileService\n",
    "\n",
    "stopwords = {\"#\", \"*\", \"--\"}\n",
    "\n",
    "cleaned_df = FileService.read_parquet_to_df(file_name='articles_cleaned')\n",
    "de_df = cleaned_df[cleaned_df['language'] == 'de']\n",
    "\n",
    "nlp_de = spacy.load('de_core_news_sm')\n",
    "nlp_de.Defaults.stop_words |= stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = nlp_de.pipe(de_df['content'], disable=[\"tagger\", \"ner\", \"textcat\"], n_process=4)\n",
    "\n",
    "# Tags to be removed: https://universaldependencies.org/u/pos/\n",
    "pos_to_remove= [\"ADP\", \"ADV\", \"AUX\", \"CCONJ\", \"DET\", \"INTJ\", \"NUM\", \"PART\", \"PRON\", \"PUNCT\", \"SCONJ\", \"SYM\", ]\n",
    "\n",
    "tokenized_articles = []\n",
    "i = 0\n",
    "for article in articles:\n",
    "   i += 1\n",
    "   if i % 1000 == 0:\n",
    "      print(f\"At step: {i} of {len(de_df)}\")\n",
    "\n",
    "   article_tokens = []\n",
    "   for token in article:\n",
    "      if (\n",
    "         token.pos_ not in pos_to_remove # Remove defined parts of speech\n",
    "         and not token.is_stop # Token is not a stopword\n",
    "         and not token.is_space\n",
    "      ):\n",
    "         article_tokens.append(token.lemma_.lower())\n",
    "\n",
    "   tokenized_articles.append(article_tokens)\n",
    "\n",
    "dictionary_german_removed_pos = corpora.Dictionary(tokenized_articles)\n",
    "dictionary_german_removed_pos.save(fname_or_handle=os.path.normpath(\"./models/dictionaries/dictionary-german-removed-pos\"))\n",
    "print(\"Exported dictionary: dictionary_german_removed_pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary_german_removed_pos.most_common(n=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionaries with only nouns\n",
    "Inspired by https://aclanthology.org/U15-1013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = nlp_de.pipe(de_df['content'], disable=[\"tagger\", \"ner\", \"textcat\"], n_process=4)\n",
    "\n",
    "# The only tag to keep is nouns: https://universaldependencies.org/u/pos/\n",
    "nouns = [\"NOUN\", \"PROPN\"]\n",
    "\n",
    "tokenized_articles = []\n",
    "i = 0\n",
    "for article in articles:\n",
    "   i += 1\n",
    "   if i % 1000 == 0:\n",
    "      print(f\"At step: {i} of {len(de_df)}\")\n",
    "\n",
    "   article_tokens = []\n",
    "   for token in article:\n",
    "      if (\n",
    "         token.pos_ in nouns\n",
    "         and not token.is_stop\n",
    "         ):\n",
    "         article_tokens.append(token.lemma_.lower())\n",
    "\n",
    "   tokenized_articles.append(article_tokens)\n",
    "\n",
    "dictionary_german_noun_only = corpora.Dictionary(tokenized_articles)\n",
    "dictionary_german_noun_only.save(fname_or_handle=os.path.normpath(\"./models/dictionaries/dictionary-german-noun-only\"))\n",
    "print(\"Exported dictionary: dictionary_german_noun-only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very few weird words in top 300:\n",
    "\n",
    "```py\n",
    "stopwords = [\"\\x96\", \"the\", \"#\", \"keystone\", \"*\", \"--\", \"a\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary_german_noun_only.most_common(n=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionaries with some POS removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "from gensim import corpora\n",
    "from pipeline.service import FileService\n",
    "\n",
    "\n",
    "cleaned_df = FileService.read_parquet_to_df(file_name='articles_cleaned')\n",
    "fr_df = cleaned_df[cleaned_df['language'] == 'fr']\n",
    "\n",
    "nlp_fr = spacy.load('fr_core_news_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = nlp_fr.pipe(fr_df['content'], disable=[\"tagger\", \"ner\", \"textcat\"], n_process=4)\n",
    "\n",
    "# Tags to be removed: https://universaldependencies.org/u/pos/\n",
    "pos_to_remove= [\"ADP\", \"ADV\", \"AUX\", \"CCONJ\", \"DET\", \"INTJ\", \"NUM\", \"PART\", \"PRON\", \"PUNCT\", \"SCONJ\", \"SYM\", ]\n",
    "stopwords = [\"«\", \"-\", \" \", \"m.\", \"#\", \"–\", \"\\x96\", \"*\", \"c\\x92est\", \"d\\x92un\", \"-t\", \"/\" ,\n",
    "             \"qu\\x92il\", \"webtv@20minutes.ch\", \"j.\", \"d\\x92autre\", \"https://t.co\", \"c\\x9cur\", \n",
    "             \"j\\x92ai\", \"h.\", \"o\", \"n\\x92er\", \"n\\x92a\", \"c.\", \"s.\", \".keystone\", \"n\\x92y\", \n",
    "             \"s\\x9cur\", \"l.\", \"b.\", \"\\x9cuvre\", \"jusqu\\x92à\", \"n\\x92aver\", \"|\", \"''\", \"n\\x92est\", \"…\"] # And many more...\n",
    "tokenized_articles = []\n",
    "i = 0\n",
    "len_df = len(fr_df)\n",
    "for article in articles:\n",
    "   i += 1\n",
    "   if i % 1000 == 0:\n",
    "      print(f\"At step: {i} of {len_df}\")\n",
    "\n",
    "   article_tokens = []\n",
    "   for token in article:\n",
    "      if (\n",
    "         token.pos_ not in pos_to_remove # Remove defined parts of speech\n",
    "         and not token.is_stop # Token is not a stopword\n",
    "         and not token.is_space\n",
    "      ):\n",
    "         article_tokens.append(token.lemma_.lower())\n",
    "\n",
    "   tokenized_articles.append(article_tokens)\n",
    "\n",
    "dictionary_french_removed_pos = corpora.Dictionary(tokenized_articles)\n",
    "dictionary_french_removed_pos.save(fname_or_handle=os.path.normpath(\"./models/dictionaries/dictionary-french-removed-pos\"))\n",
    "print(\"Exported dictionary: dictionary_french_removed_pos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still contains some \n",
    "* special characters\n",
    "* abbreviations \"a.\", \"l.\" -> might this be the end of a sentence? \n",
    "* a lot of escaped \"\\x92\"... this might be apostrophes -> encoding issue during preprocessing?\n",
    "\n",
    "```py\n",
    "stopwords = [\"«\", \"-\", \" \", \"m.\", \"#\", \"–\", \"\\x96\", \"*\", \"c\\x92est\", \"d\\x92un\", \"-t\", \"/\" ,\n",
    "\"qu\\x92il\", \"webtv@20minutes.ch\", \"j.\", \"d\\x92autre\", \"https://t.co\", \"c\\x9cur\", \n",
    "\"j\\x92ai\", \"h.\", \"o\", \"n\\x92er\", \"n\\x92a\", \"c.\", \"s.\", \".keystone\", \"n\\x92y\", \n",
    "\"s\\x9cur\", \"l.\", \"b.\", \"\\x9cuvre\", \"jusqu\\x92à\", \"n\\x92aver\", \"|\", \"''\", \"n\\x92est\", \"…\"] # And many more...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary_french_removed_pos.most_common(n=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionaries with only nouns\n",
    "Inspired by https://aclanthology.org/U15-1013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = nlp_fr.pipe(fr_df['content'], disable=[\"tagger\", \"ner\", \"textcat\"], n_process=4)\n",
    "\n",
    "# The only tag to keep is nouns: https://universaldependencies.org/u/pos/\n",
    "nouns = [\"NOUN\", \"PROPN\"]\n",
    "stopwords = [\"«\", \"-\", \"l’\", \"afp\", \"m.\", \"a\", \"#\"] # Top 300\n",
    "tokenized_articles = []\n",
    "i = 0\n",
    "len_df = len(fr_df)\n",
    "for article in articles:\n",
    "   i += 1\n",
    "   if i % 1000 == 0:\n",
    "      print(f\"At step: {i} of {len_df}\")\n",
    "\n",
    "   article_tokens = []\n",
    "   for token in article:\n",
    "      if (\n",
    "         token.pos_ in nouns\n",
    "         and not token.is_stop\n",
    "         ):\n",
    "         article_tokens.append(token.lemma_.lower())\n",
    "\n",
    "   tokenized_articles.append(article_tokens)\n",
    "\n",
    "dictionary_french_noun_only = corpora.Dictionary(tokenized_articles)\n",
    "dictionary_french_noun_only.save(fname_or_handle=os.path.normpath(\"./models/dictionaries/dictionary-french-noun-only\"))\n",
    "print(\"Exported dictionary: dictionary_french_noun-only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very few weird words in top 300:\n",
    "```py\n",
    "stopwords = [\"«\", \"-\", \"l’\", \"afp\", \"m.\", \"a\", \"#\"] # Top 300\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary_french_noun_only.most_common(n=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
