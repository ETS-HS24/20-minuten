{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from top2vec import Top2Vec\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "from pipeline.service._file import FileService\n",
    "\n",
    "df = FileService.read_parquet_to_df(\"articles_lemmatized\")\n",
    "print(f\"samples: {len(df)}\")\n",
    "\n",
    "umap_args = {\n",
    "    \"n_neighbors\": 55,\n",
    "    \"n_components\": 15,\n",
    "    \"metric\": \"cosine\"\n",
    "}\n",
    "hdbscan_args = {\n",
    "    'min_cluster_size': 20,\n",
    "    'metric': \"euclidean\",\n",
    "    'cluster_selection_method': \"eom\"\n",
    "}\n",
    "\n",
    "\n",
    "pretrained_model = SentenceTransformer('sentence-transformers/LaBSE')  \n",
    "model = Top2Vec(\n",
    "        documents=list(df[\"content\"]), \n",
    "        embedding_model=pretrained_model.encode, \n",
    "        chunk_length=pretrained_model.max_seq_length, \n",
    "        chunk_overlap_ratio=.2,\n",
    "        gpu_umap=False,\n",
    "        umap_args=umap_args,\n",
    "        gpu_hdbscan=False,\n",
    "        hdbscan_args=hdbscan_args,\n",
    "        speed=\"learn\", \n",
    "        workers=4,\n",
    "        verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "#model.save(os.path.normpath(\"./models/top2vec/labse-full-optimized\"))\n",
    "\n",
    "embeddings = model.document_vectors\n",
    "#np.save(os.path.normpath(\"./models/vectorspaces/jan-jun-2020-embeddings.npy\"), embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from top2vec import Top2Vec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "\n",
    "pretrained_model = SentenceTransformer('sentence-transformers/LaBSE')  \n",
    "model2 = Top2Vec.load(os.path.normpath(\"./models/top2vec/labse-full-optimized\"))\n",
    "model2.set_embedding_model(pretrained_model)\n",
    "model2.get_num_topics()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After optimization:**  \n",
    "93 Topics for german + french instead of over 180 for just french"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to optimize umap after the fact (outdated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = model2.document_vectors\n",
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "\n",
    "umap_model = umap.UMAP(n_neighbors=5, n_components=2, metric='cosine', verbose=False).fit(v)\n",
    "reduced2d = umap_model.transform(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x, y = reduced2d[:,0], reduced2d[:,1]\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "\n",
    "v_pos = abs(np.min(v)) + v\n",
    "#v_pos = v\n",
    "umap_model = umap.UMAP(min_dist=0.1 ,n_neighbors=15, n_components=3, metric='hellinger', verbose=False).fit(v_pos)\n",
    "reduced3d = umap_model.transform(v_pos)\n",
    "\n",
    "# Prepare 3D graph\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "# Plot scaled features\n",
    "xdata = reduced3d[:,0]\n",
    "ydata = reduced3d[:,1]\n",
    "zdata = reduced3d[:,2]\n",
    "\n",
    "# Plot 3D plot\n",
    "ax.scatter3D(xdata, ydata, zdata, c=zdata, cmap='viridis')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(ydata > 8))\n",
    "print(sum(xdata > 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import tpe, hp, fmin, STATUS_OK, Trials, rand\n",
    "import logging\n",
    "import umap\n",
    "import numpy as np\n",
    "import hdbscan\n",
    "\n",
    "from top2vec import Top2Vec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout,\n",
    "                    level=logging.INFO,\n",
    "                    format='%(asctime)s | [%(filename)s:%(lineno)d] %(levelname)s | %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "#dataset_name = \"./models/top2vec/labse-fr-dataset\"\n",
    "dataset_name = \"./models/top2vec/labse-full-dataset\"\n",
    "\n",
    "pretrained_model = SentenceTransformer('sentence-transformers/LaBSE', device=\"cpu\")  \n",
    "model2 = Top2Vec.load(os.path.normpath(dataset_name))\n",
    "model2.set_embedding_model(pretrained_model)\n",
    "print(\"topics of loaded model\", model2.get_num_topics())\n",
    "loaded_embeddings = model2.document_vectors\n",
    "\n",
    "\n",
    "def hyperparameter_tuning(params, embeddings=loaded_embeddings, target_clusters=30, threshold_labels=50):\n",
    "    umap_args = {\n",
    "        \"n_neighbors\": params[\"n_neighbors\"],\n",
    "        \"n_components\": params[\"n_components\"],\n",
    "        \"metric\": params[\"umap_metric\"]\n",
    "    }\n",
    "    hdbscan_args = {\n",
    "        'min_cluster_size': params[\"min_cluster_size\"],\n",
    "        'metric': params[\"hdbscan_metric\"],\n",
    "        'cluster_selection_method': params[\"cluster_selection_method\"]\n",
    "    }\n",
    "    \n",
    "    if params[\"umap_metric\"] == \"hellinger\":\n",
    "        embeddings = abs(np.min(embeddings)) + embeddings \n",
    "    \n",
    "\n",
    "    logging.debug(f\"starting umap fitting\")\n",
    "    logging.debug(umap_args)\n",
    "    clustered_embeddings = umap.UMAP(**umap_args).fit_transform(embeddings)\n",
    "\n",
    "    logging.debug(f\"starting dbscan fitting\")\n",
    "    logging.debug(hdbscan_args)\n",
    "    cluster_labler = hdbscan.HDBSCAN(**hdbscan_args).fit(clustered_embeddings)\n",
    "\n",
    "    labels = cluster_labler.labels_\n",
    "\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    clusters = dict(zip(unique, counts))\n",
    "    sorted_labels = dict(sorted(clusters.items(), key=lambda item: item[1], reverse=True))\n",
    "    n_labels = len({k:v for k, v in sorted_labels.items() if v > threshold_labels}) # don't count labels below threshold\n",
    "    logging.debug(f\"Found {n_labels} labels\")\n",
    "    metric = np.abs(target_clusters - n_labels)\n",
    "\n",
    "\n",
    "    return {\"n_labels\": n_labels, \"status\": STATUS_OK, \"loss\": metric, \"below_threshold\": len(sorted_labels) - n_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trials object\n",
    "\n",
    "n_neighbors = [20, 25, 40, 45, 55]\n",
    "n_components = [5, 9, 13, 17, 20, 23]\n",
    "umap_metric = [\"hellinger\", \"cosine\"]\n",
    "min_cluster_size = [10, 15, 20]\n",
    "hdbscan_metric = [\"euclidean\"]\n",
    "cluster_selection_method = [\"eom\"]\n",
    "\n",
    "space = {\n",
    "    \"n_neighbors\": hp.choice(\"n_neighbors\", n_neighbors),\n",
    "    \"n_components\": hp.choice(\"n_components\", n_components),\n",
    "    \"umap_metric\": hp.choice(\"umap_metric\", umap_metric),\n",
    "    \"min_cluster_size\": hp.choice(\"min_cluster_size\", min_cluster_size),\n",
    "    \"hdbscan_metric\": hp.choice(\"hdbscan_metric\", hdbscan_metric), \n",
    "    \"cluster_selection_method\": hp.choice(\"cluster_selection_method\", cluster_selection_method)\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(\n",
    "    fn=hyperparameter_tuning,\n",
    "    space = space, \n",
    "    algo=tpe.rand.suggest, #tpe.suggest, \n",
    "    max_evals=200, \n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "print(\"Best: {}\".format(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import space_eval\n",
    "space_eval(space, best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "#pprint([t for t in trials][0][\"result\"])\n",
    "\n",
    "def unpack(x):\n",
    "    if x:\n",
    "        return x[0]\n",
    "    return np.nan\n",
    "\n",
    "trials_df = pd.DataFrame([pd.Series(t[\"misc\"][\"vals\"]).apply(unpack) for t in trials])\n",
    "trials_df[\"loss\"] = [t[\"result\"][\"loss\"] for t in trials]\n",
    "trials_df[\"n_labels\"] = [t[\"result\"][\"n_labels\"] for t in trials]\n",
    "trials_df[\"below_threshold\"] = [t[\"result\"][\"below_threshold\"] for t in trials]\n",
    "\n",
    "trials_df[\"n_neighbors\"] = trials_df[\"n_neighbors\"].replace(list(range(len(n_neighbors))), n_neighbors)\n",
    "trials_df[\"n_components\"] = trials_df[\"n_components\"].replace(list(range(len(n_components))), n_components)\n",
    "trials_df[\"umap_metric\"] = trials_df[\"umap_metric\"].replace(list(range(len(umap_metric))), umap_metric)\n",
    "trials_df[\"min_cluster_size\"] = trials_df[\"min_cluster_size\"].replace(list(range(len(min_cluster_size))), min_cluster_size)\n",
    "trials_df[\"hdbscan_metric\"] = trials_df[\"hdbscan_metric\"].replace(list(range(len(hdbscan_metric))), hdbscan_metric)\n",
    "trials_df[\"cluster_selection_method\"] = trials_df[\"cluster_selection_method\"].replace(list(range(len(cluster_selection_method))), cluster_selection_method)\n",
    "\n",
    "trials_df.sort_values(by=\"loss\", inplace=True)\n",
    "\n",
    "experiment_params = {\n",
    "    \"n_neighbors\": n_neighbors,\n",
    "    \"n_components\": n_components,\n",
    "    \"umap_metric\": umap_metric,\n",
    "    \"min_cluster_size\": min_cluster_size,\n",
    "    \"hdbscan_metric\": hdbscan_metric,\n",
    "    \"cluster_selection_method\": cluster_selection_method,\n",
    "}\n",
    "\n",
    "experiment_start = [t for t in trials][0][\"book_time\"]\n",
    "timestamp = experiment_start.strftime('%Y-%m-%d-%H-%M-%S')\n",
    "experiment_folder = os.path.normpath(f\"./umap-hdbet-hyperopt-{timestamp}/\")\n",
    "Path(experiment_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(experiment_folder, \"dataset-path.txt\"), \"w\") as f:\n",
    "    f.write(dataset_name)\n",
    "\n",
    "with open(os.path.join(experiment_folder, \"optimization-space.json\"), \"w\") as f:\n",
    "    json.dump(experiment_params, f)\n",
    "\n",
    "trials_df.to_csv(os.path.join(experiment_folder, \"optimization-results.csv\"))\n",
    "\n",
    "trials_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload from disk\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "df = pd.read_csv(os.path.normpath(\"./umap-hdbet-hyperopt-2024-11-10-15-04-29/optimization-results.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_df.plot(x=\"n_neighbors\", y=\"loss\", style=\"o\", title=\"Number of neighbors vs. loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_df.plot(x=\"n_components\", y=\"loss\", style=\"o\", title=\"Number of Components vs. loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_df.plot(x=\"min_cluster_size\", y=\"loss\", style=\"o\", title=\"Minimum cluster size vs. loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trials_df[\"umap_metric\"].value_counts())\n",
    "print(trials_df.iloc[:50][\"umap_metric\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "correlation = trials_df[[\"min_cluster_size\", \"n_components\", \"n_neighbors\", \"loss\"]].corr()\n",
    "correlation.style.background_gradient(cmap='coolwarm').format(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best result is:\n",
    "```py\n",
    "{\n",
    "    'cluster_selection_method': 'eom',\n",
    "    'hdbscan_metric': 'euclidean',\n",
    "    'min_cluster_size': 15,\n",
    "    'n_components': 13,\n",
    "    'n_neighbors': 40,\n",
    "    'umap_metric': 'cosine'\n",
    "}\n",
    "```\n",
    "\n",
    "Within the search space, all varibles have a negative correlation with the loss. As the variable increases the loss decreases. \n",
    "\n",
    "The custom loss function was to come close to approximately 30 labels since there are around 29 topics shown on the 20min webpage.  \n",
    "For the best result we have ``n_neighbors = 40``, but if we inspect the top 10 results they tend to go higher. This confirms the intuition that we can reduce the number of labels by forcibly increasing the number of required neighbors. We shall choose ``55`` as we increase the corpus.   \n",
    "The correlation for the number of `components` is very week and within the search space not significant. In the top 10 results the mean is `14.1` which is very close to the search space mean of `14.5`. We shall pick any number within the space.  \n",
    "The UMAP `metric` is quite balanced, within the top 10 as well as the whole search space. We shall choose `cosine` as this will behave better for sparser datasets and is understood to be regularly used for NLP.  \n",
    "The top 10 `min_cluster_size` with a mean of 17.5 is close to the search space mean of 15. We shall choose ``20`` as we increase the corpus.  \n",
    "\n",
    "With this variable selection we can see that it would fit the second best result which also has the lowest number of labels below the threshold. So this can indicate more cohesion within the HDBSCAN labels.\n",
    "\n",
    "The model shall be fitted with the following parameters:\n",
    "```python\n",
    "umap_args = {\n",
    "    \"n_neighbors\": 55,\n",
    "    \"n_components\": 15,\n",
    "    \"metric\": \"cosine\"\n",
    "}\n",
    "hdbscan_args = {\n",
    "    'min_cluster_size': 20,\n",
    "    'metric': \"euclidean\",\n",
    "    'cluster_selection_method': \"eom\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_df.iloc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_num_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_sizes, topic_nums = model.get_topic_sizes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words, word_scores, topic_nums = model.get_topics(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words, word_scores, topic_scores, topic_nums = model.search_topics(keywords=[\"allemand\"], num_topics=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topic_words, topic_scores, topic_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduced_hierarchy = model.hierarchical_topic_reduction(75, interval=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topics in enumerate(reduced_hierarchy):\n",
    "    if 187 in topics:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate_topic_wordcloud(86)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words, word_scores, topic_scores, topic_nums = model.search_topics(keywords=[\"crypto\"], num_topics=3)\n",
    "for topic in topic_nums:\n",
    "    model.generate_topic_wordcloud(topic, reduced=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = FileService.read_parquet_to_df(\"articles_cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents, document_scores, document_ids = model.search_documents_by_topic(topic_num=20, num_docs=129)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[5538]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
